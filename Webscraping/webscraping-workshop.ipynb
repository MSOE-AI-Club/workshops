{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Webscraping\n",
    "===\n",
    "MAIC - Spring, Week 3<br>\n",
    "```\n",
    "  _____________\n",
    " /0   /     \\  \\\n",
    "/  \\ M A I C/  /\\\n",
    "\\ / *      /  / /\n",
    " \\___\\____/  @ /\n",
    "          \\_/_/\n",
    "```\n",
    "(Rosie is not needed!)\n",
    "\n",
    "Prereqs:\n",
    "- Install [VSCode](https://code.visualstudio.com/)\n",
    "- Install [Python](https://www.python.org/downloads/)\n",
    "  - \"The best language\" - Guido van Rossum\n",
    "- Ensure you can run notebooks in VSCode.\n",
    "\n",
    "<span style=\"color:#ff5555;font-weight:bold;font-size:1.5rem;\">\n",
    "    STOP\n",
    "</span>\n",
    "\n",
    "... or keep going if you want to work ahead.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is webscraping?**\n",
    "\n",
    "Are you in need of data? Maybe you want to analyze some data for insights. Or maybe you just want to train a model. In any case, you may be able to get the data you need via webscraping!\n",
    "\n",
    "Webscraping is the process of *automatically* extracting data from websites. You can manually extract website data on your browser via \"inspect,\" but automating this process is ideal if you need anything more than a few samples.\n",
    "\n",
    "- Go to any website (for instance, the [MAIC](https://msoe-maic.com/) site).\n",
    "- Right-click anywhere on the page. Select the \"inspect\" option or something labeled similarly. This is usually at the bottom of the pop-up menu.\n",
    "- Note the window that opened. It contains the raw HTML (and possibly JS/CSS) site data. This is what we want to scrape automatically.\n",
    "- Use the element selector at the top left of the inspect window to see the HTML of specific elements.\n",
    "\n",
    "---\n",
    "\n",
    "<span style=\"color:#55ff55;font-weight:bold;font-size:1.5rem;\">\n",
    "    GO\n",
    "</span>\n",
    "\n",
    "**That's cool. How can I scrape automatically?**\n",
    "\n",
    "Let's try scraping the MAIC leaderboard!\n",
    "\n",
    "Basic scraping only needs the `requests` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html class=\"client-nojs vector-feature-language-in-header-enabled vector-feature-language-in-main-page-header-disabled vector-feature-page-tools-pinned-disabled vector-feature-toc-pinned-clientpref-1 vector-feature-main-menu-pinned-disabled vector-feature-limited-width-clientpref-1 vector-feature-limited-width-content-enabled vector-feature-custom-font-size-clientpref-1 vector-feature-appearance-pinned-clientpref-1 vector-feature-night-mode-enabled skin-theme-clientpref-day vect\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "URL = 'https://msoe-maic.com'\n",
    "\n",
    "html = requests.get(URL).text # Make a request to the URL and get the HTML via `.text`\n",
    "print(html[:500]) # Print some of the resulting HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This html now contains the leaderboard for us to extract. But how do we extract it?\n",
    "\n",
    "One easy way is to *inspect* the page on your browser, and to see if the HTML can easily identify the leaderboard. It seems that the leaderboard element is in the \"leaderboard-table\" class:\n",
    "\n",
    "```html\n",
    "<table border=\"1\" class=\"dataframe leaderboard-table\" id=\"df_data\">\n",
    "    ...\n",
    "</table>\n",
    "```\n",
    "\n",
    "We could try looking for \"leaderboad-table\" in the html string, but there's a better way. `Beautifulsoup` is a Python library that makes parsing HTML easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install BeautifulSoup and possibly restart your notebook, being sure to re-run prior cells.\n",
    "%pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup # We can now use BeautifulSoup to parse the HTML\n",
    "\n",
    "soup = BeautifulSoup(html, 'html.parser') \n",
    "print(soup.prettify()[:500]) # print it as before, but now it's prettified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use BeautifulSoup to find the \"leaderboard-table\" element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the table element with class \"leaderboard-table\"\n",
    "\n",
    "leaderboard_table = soup.find('table', {'class': 'leaderboard-table'})\n",
    "\n",
    "print(leaderboard_table.prettify()[:500]) # print the first 500 characters of the table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not only can Beautifulsoup find the element, it also allows us to easily extract the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract table data into a list of dictionaries\n",
    "\n",
    "rows = leaderboard_table.find_all('tr') # Find all rows in the table\n",
    "header = [cell.text for cell in rows[0].find_all('th')] # Get the header row\n",
    "data = [\n",
    "    {header[i]: cell.text for i, cell in enumerate(row.find_all('td'))} # Create a dictionary for each row using the header to name the keys\n",
    "    for row in rows[1:]\n",
    "]\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty neat, right?\n",
    "\n",
    "<span style=\"color:#ff5555;font-weight:bold;font-size:1.5rem;\">\n",
    "    STOP\n",
    "</span>\n",
    "\n",
    "... or keep going if you want to work ahead.\n",
    "\n",
    "---\n",
    "\n",
    "**It's not always this easy.**\n",
    "\n",
    "Some pages dynamically generate content using Javascript. This is a problem for us because the `requests` library cannot run Javascript code. Let's try to scrape content from a page that uses a lot of Javascript.\n",
    "\n",
    "- Go to [the MAIC research groups page](https://msoe-maic.com/library?nav=Research).\n",
    "- Use the element selector to select a group's section.\n",
    "- Note the id of the element.\n",
    "\n",
    "For instance, the page has this div with an id of `agent-simulation-experts`.\n",
    "\n",
    "```html\n",
    "<div class=\"MuiPaper-root MuiPaper-elevation MuiPaper-rounded MuiPaper-elevation1 MuiCard-root modal css-1kil0ip\" id=\"agent-simulation-experts\">\n",
    "    ...\n",
    "</div>\n",
    "```\n",
    "\n",
    "It's important to note, however, that this element was generated with Javascript. So what happens if we try scraping this element with `requests`?\n",
    "\n",
    "<span style=\"color:#55ff55;font-weight:bold;font-size:1.5rem;\">\n",
    "    GO\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "URL = 'https://msoe-maic.com/library?nav=Research'\n",
    "\n",
    "html = requests.get(URL).text # Make a request to the URL and get the HTML via `.text`\n",
    "\n",
    "# We don't seem to get much HTML from this page\n",
    "print(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In fact, the HTML has zero mentions of the div we saw earlier!\n",
    "print('agent-simulation-experts' in html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<span style=\"color:#5555ff;font-weight:bold;\">\n",
    "    Try this yourself.\n",
    "</span>\n",
    "\n",
    "Go to some websites and see what HTML you can scrape with `requests`. See if anything in the browser inspection tool appears in the `html` variable. You may find that a majority of websites aren't easily scrapable.\n",
    "\n",
    "Some sites to try:\n",
    "- https://www.youtube.com/\n",
    "- https://www.google.com/search?q=your+search+here\n",
    "- https://www.reddit.com/\n",
    "- https://stackoverflow.com/questions\n",
    "- https://github.com/MSOE-AI-Club\n",
    "\n",
    "<span style=\"color:#ff5555;font-weight:bold;font-size:1.5rem;\">\n",
    "    STOP\n",
    "</span>\n",
    "\n",
    "... or keep going if you want to work ahead.\n",
    "\n",
    "---\n",
    "\n",
    "**Q: So how do we scrape pages that use Javascript?**\n",
    "\n",
    "A: Use Selenium.\n",
    "\n",
    "Selenium is a headless browser that can execute page Javascript.\n",
    "\n",
    "the `requests` library cannot run Javascript, so any page content generated by said Javascript is impossible to scrape with `requests` alone. Luckily, browsers are *made* to run Javascript. Selinum runs javascript like a regular browser (and it even uses a regular browser such as Chrome under the hood), but it functions without a UI so you can interact with pages programatically\n",
    "\n",
    "\n",
    "<span style=\"color:#55ff55;font-weight:bold;font-size:1.5rem;\">\n",
    "    GO\n",
    "</span>\n",
    "\n",
    "We'll wrap `selenium` in a function call to make it work similarly to `requests`. Feel free to read the function comments if you want to dive deeper into `selenium`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install selenium\n",
    "%pip install webdriver-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from selenium.webdriver.edge.service import Service as EdgeService\n",
    "from selenium.webdriver.chrome.options import Options as ChromeOptions\n",
    "from selenium.webdriver.edge.options import Options as EdgeOptions\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from webdriver_manager.microsoft import EdgeChromiumDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "def setup_chrome():\n",
    "    options = ChromeOptions()\n",
    "    options.add_argument('--headless')  # Run in headless mode (no GUI)\n",
    "    options.add_argument('--disable-gpu')\n",
    "    options.add_argument('--no-sandbox')\n",
    "    options.add_argument('--disable-dev-shm-usage')\n",
    "    \n",
    "    print('Opening Chrome Webdriver')\n",
    "    return webdriver.Chrome(\n",
    "        service=ChromeService(ChromeDriverManager().install()),\n",
    "        options=options\n",
    "    )\n",
    "        \n",
    "def setup_edge():\n",
    "    options = EdgeOptions()\n",
    "    options.add_argument('--headless')\n",
    "    options.add_argument('--disable-gpu')\n",
    "    options.add_argument('--no-sandbox')\n",
    "    options.add_argument('--disable-dev-shm-usage')\n",
    "    \n",
    "    print('Opening Edge Webdriver')\n",
    "    return webdriver.Edge(\n",
    "        service=EdgeService(EdgeChromiumDriverManager().install()),\n",
    "        options=options\n",
    "    )\n",
    "\n",
    "driver = None\n",
    "try:\n",
    "    driver = setup_chrome()\n",
    "except Exception as e:\n",
    "    print(f\"Chrome failed\")\n",
    "    print(\"Falling back to Edge...\")\n",
    "    driver = setup_edge()\n",
    "\n",
    "def get_page_content(url):\n",
    "    \"\"\"\n",
    "    Opens a URL using Selenium and retrieves the page contents.\n",
    "    Tries Chrome first, falls back to Edge if Chrome fails.\n",
    "    \n",
    "    Args:\n",
    "        url (str): The URL to open\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (raw_html, parsed_text) where raw_html is the page source and \n",
    "               parsed_text is the cleaned text content\n",
    "    \"\"\"\n",
    "    \n",
    "    print('Scrape')\n",
    "    \n",
    "    try:\n",
    "        # Open the URL\n",
    "        driver.get(url)\n",
    "        \n",
    "        # Wait for a short time to ensure the page loads\n",
    "        time.sleep(2)\n",
    "        \n",
    "        # Get the page source\n",
    "        page_content = driver.page_source\n",
    "        \n",
    "        # Parse with BeautifulSoup\n",
    "        soup = BeautifulSoup(page_content, 'html.parser')\n",
    "        \n",
    "        # Remove script and style elements\n",
    "        for script in soup([\"script\", \"style\"]):\n",
    "            script.decompose()\n",
    "        \n",
    "        # Find all code blocks and wrap their text in backticks\n",
    "        code_blocks = soup.find_all(['pre', 'code'])\n",
    "        for block in code_blocks:\n",
    "            if block.string:\n",
    "                block.string = f'```{block.string}```'\n",
    "            else:\n",
    "                # Handle nested elements within code blocks\n",
    "                block.string = f'```{block.get_text()}```'\n",
    "                \n",
    "        # Get text and clean it\n",
    "        text = soup.get_text().replace(\"```Copy\", \"```\")\n",
    "        \n",
    "        # Clean up the text\n",
    "        # Break into lines and remove leading/trailing space on each\n",
    "        lines = (line.strip() for line in text.splitlines())\n",
    "        \n",
    "        # Break multi-headlines into a line each\n",
    "        chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "        \n",
    "        # Drop blank lines\n",
    "        text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
    "        \n",
    "        return page_content, text\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'https://msoe-maic.com/library?nav=Research'\n",
    "\n",
    "# YOU NEED Chrome or Edge installed. Sorry Mac users :(\n",
    "html, _ = get_page_content(URL) # Get the page html, but this time with selenium. NOTE: This will take a while to run the first time around b/c the webdriver has to be installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the element with id 'agent-simulation-experts' and then find any descendant with class 'modal-header'\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "agent_sim_div = soup.find('div', {'id': 'agent-simulation-experts'})\n",
    "modal_header = agent_sim_div.find(class_='modal-header')\n",
    "print(modal_header.get_text().strip() if modal_header else \"No modal-header found\")\n",
    "print(agent_sim_div)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#ff5555;font-weight:bold;font-size:1.5rem;\">\n",
    "    STOP\n",
    "</span>\n",
    "\n",
    "... or keep going if you want to work ahead.\n",
    "\n",
    "---\n",
    "\n",
    "**Using LLMs to summarize scraped data.**\n",
    "\n",
    "If you're scraping unstructured data, then LLMs are a must. Although there is structure in the HTML elements, it can often be easier to ask an LLM to structure the output for you.\n",
    "\n",
    "Let's structure the output of a page listing refurbished iPhones for sale.\n",
    "\n",
    "You will need Gemini API keys to run this example. [Link to Gemini API](https://aistudio.google.com/)\n",
    "\n",
    "<span style=\"color:#55ff55;font-weight:bold;font-size:1.5rem;\">\n",
    "    GO\n",
    "</span>\n",
    "\n",
    "This example will:\n",
    "- Use Selenium to scrape for refurbished iPhones.\n",
    "- Use an LLM to summarize the results into a structured format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"You're doing great kid\" - Linus Torvalds\n",
    "%pip install pydantic pydantic-ai pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pydantic_ai import Agent\n",
    "from typing import List\n",
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'https://www.apple.com/shop/refurbished/iphone/iphone-14-pro'\n",
    "\n",
    "# --- NOTE: put your key here ---\n",
    "os.environ[\"GEMINI_API_KEY\"] = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is where you can specify the output structure\n",
    "\n",
    "class ProductResult(BaseModel):  \n",
    "    model: str = Field(description='The model of the product')\n",
    "    description: str = Field(description='The description of the product')\n",
    "    cost: int = Field(description=\"The cost of the product\")\n",
    "    isp: str = Field(description=\"The internet service provider\")\n",
    "    color: str = Field(description=\"The color of the product\")\n",
    "    refurbished: bool = Field(description=\"Whether the product is refurbished\")\n",
    "\n",
    "# We are storing a list of ProductResults in the final output\n",
    "class RequestResults(BaseModel):\n",
    "    products: List[ProductResult] = Field(description='The list of product results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Here's where the AI comes in\" - Andrej Karpathy\n",
    "agent = Agent( # Create an agent that will structure the output\n",
    "    'google-gla:gemini-1.5-flash',\n",
    "    result_type=RequestResults,\n",
    "    system_prompt='Be concise, reply with one sentence.',  \n",
    ")\n",
    "\n",
    "# Agent system prompt - tell it what to do\n",
    "@agent.system_prompt  \n",
    "async def add_customer_name(ctx) -> str:\n",
    "    return f\"Your goal is to extract product information from web scraped pages and format it to a structured response.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, text = get_page_content(URL) # Scrape the list of iPhones, and get the text (so the LLM can read it more easily)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = await agent.run(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "# Structure the output into a DataFrame to see our results\n",
    "item_dicts = [item.model_dump() for item in result.data.products]\n",
    "df = pd.DataFrame(item_dicts)\n",
    "display(df.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit() # Always remember to close the webdriver when you're done with it"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
