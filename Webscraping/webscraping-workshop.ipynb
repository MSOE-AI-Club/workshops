{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Webscraping\n",
    "===\n",
    "MAIC - Spring, Week 2<br>\n",
    "```\n",
    "  _____________\n",
    " /0   /     \\  \\\n",
    "/  \\ M A I C/  /\\\n",
    "\\ / *      /  / /\n",
    " \\___\\____/  @ /\n",
    "          \\_/_/\n",
    "```\n",
    "(Rosie is not needed!)\n",
    "\n",
    "Prereqs:\n",
    "- Install [VSCode](https://code.visualstudio.com/)\n",
    "- Install [Python](https://www.python.org/downloads/)\n",
    "- Ensure you can run notebooks in VSCode.\n",
    "\n",
    "<span style=\"color:#ff5555;font-weight:bold;font-size:1.5rem;\">\n",
    "    STOP\n",
    "</span>\n",
    "\n",
    "... or keep going if you want to work ahead.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is webscraping?**\n",
    "\n",
    "Are you in need of data? Maybe you want to analyze some data for insights. Or maybe you just want to train a model. In any case, you may be able to get the data you need via webscraping!\n",
    "\n",
    "Webscraping is the process of *automatically* extracting data from websites. You can manually extract website data on your browser via \"inspect,\" but automating this process is ideal if you need anything more than a few samples.\n",
    "\n",
    "- Go to any website (for instance, the [MAIC](https://msoe-maic.com/) site).\n",
    "- Right-click anywhere on the page. Select the \"inspect\" option or something labeled similarly. This is usually at the bottom of the pop-up menu.\n",
    "- Note the window that opened. It contains the raw HTML (and possibly JS/CSS) site data. This is what we want to scrape automatically.\n",
    "- Use the element selector at the top left of the inspect window to see the HTML of specific elements.\n",
    "\n",
    "---\n",
    "\n",
    "<span style=\"color:#55ff55;font-weight:bold;font-size:1.5rem;\">\n",
    "    GO\n",
    "</span>\n",
    "\n",
    "**That's cool. How can I scrape automatically?**\n",
    "\n",
    "Let's try scraping the MAIC leaderboard!\n",
    "\n",
    "Basic scraping only needs the `requests` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "URL = 'https://msoe-maic.com'\n",
    "\n",
    "html = requests.get(URL).text # Make a request to the URL and get the HTML via `.text`\n",
    "print(html[:500]) # Print some of the resulting HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This html now contains the leaderboard for us to extract. But how do we extract it?\n",
    "\n",
    "One easy way is to *inspect* the page on your browser, and to see if the HTML can easily identify the leaderboard. It seems that the leaderboard element is in the \"leaderboard-table\" class:\n",
    "\n",
    "```html\n",
    "<table border=\"1\" class=\"dataframe leaderboard-table\" id=\"df_data\">\n",
    "    ...\n",
    "</table>\n",
    "```\n",
    "\n",
    "We could try looking for \"leaderboad-table\" in the html string, but there's a better way. `Beautifulsoup` is a Python library that makes parsing HTML easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install beautifulsoup4 # Install BeautifulSoup and possibly restart your notebook, being sure to re-run prior cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup # We can now use BeautifulSoup to parse the HTML\n",
    "\n",
    "soup = BeautifulSoup(html, 'html.parser') \n",
    "print(soup.prettify()[:500]) # print it as before, but now it's prettified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use BeautifulSoup to find the \"leaderboard-table\" element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the table element with class \"leaderboard-table\"\n",
    "\n",
    "leaderboard_table = soup.find('table', {'class': 'leaderboard-table'})\n",
    "\n",
    "print(leaderboard_table.prettify()[:500]) # print the first 500 characters of the table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not only can Beautifulsoup find the element, it also allows us to easily extract the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract table data into a list of dictionaries\n",
    "\n",
    "rows = leaderboard_table.find_all('tr') # Find all rows in the table\n",
    "header = [cell.text for cell in rows[0].find_all('th')] # Get the header row\n",
    "data = [\n",
    "    {header[i]: cell.text for i, cell in enumerate(row.find_all('td'))} # Create a dictionary for each row using the header to name the keys\n",
    "    for row in rows[1:]\n",
    "]\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty neat, right?\n",
    "\n",
    "<span style=\"color:#ff5555;font-weight:bold;font-size:1.5rem;\">\n",
    "    STOP\n",
    "</span>\n",
    "\n",
    "... or keep going if you want to work ahead.\n",
    "\n",
    "---\n",
    "\n",
    "**It's not always this easy.**\n",
    "\n",
    "Some pages dynamically generate content using Javascript. This is a problem for us because the `requests` library cannot run Javascript code. Let's try to scrape content from a page that uses a lot of Javascript.\n",
    "\n",
    "- Go to [the MAIC research groups page](https://msoe-maic.com/library?nav=Research).\n",
    "- Use the element selector to select a group's section.\n",
    "- Note the id of the element.\n",
    "\n",
    "For instance, the page has this div with an id of `agent-simulation-experts`.\n",
    "\n",
    "```html\n",
    "<div class=\"MuiPaper-root MuiPaper-elevation MuiPaper-rounded MuiPaper-elevation1 MuiCard-root modal css-1kil0ip\" id=\"agent-simulation-experts\">\n",
    "    ...\n",
    "</div>\n",
    "```\n",
    "\n",
    "It's important to note, however, that this element was generated with Javascript. So what happens if we try scraping this element with `requests`?\n",
    "\n",
    "<span style=\"color:#55ff55;font-weight:bold;font-size:1.5rem;\">\n",
    "    GO\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "URL = 'https://msoe-maic.com/library?nav=Research'\n",
    "\n",
    "html = requests.get(URL).text # Make a request to the URL and get the HTML via `.text`\n",
    "\n",
    "# We don't seem to get much HTML from this page\n",
    "print(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In fact, the HTML has zero mentions of the div we saw earlier!\n",
    "print('agent-simulation-experts' in html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<span style=\"color:#5555ff;font-weight:bold;\">\n",
    "    Try this yourself.\n",
    "</span>\n",
    "\n",
    "Go to some websites and see what HTML you can scrape with `requests`. See if anything in the browser inspection tool appears in the `html` variable. You may find that a majority of websites aren't easily scrapable.\n",
    "\n",
    "<span style=\"color:#ff5555;font-weight:bold;font-size:1.5rem;\">\n",
    "    STOP\n",
    "</span>\n",
    "\n",
    "... or keep going if you want to work ahead.\n",
    "\n",
    "---\n",
    "\n",
    "**Q: So how do we scrape pages that use Javascript?**\n",
    "\n",
    "A: Use Selenium.\n",
    "\n",
    "Selenium is a headless browser that can execute page Javascript.\n",
    "\n",
    "the `requests` library cannot run Javascript, so any page content generated by said Javascript is impossible to scrape with `requests` alone. Luckily, browsers are *made* to run Javascript. Selinum runs javascript like a regular browser (and it even uses a regular browser such as Chrome under the hood), but it functions without a UI so you can interact with pages programatically\n",
    "\n",
    "\n",
    "<span style=\"color:#55ff55;font-weight:bold;font-size:1.5rem;\">\n",
    "    GO\n",
    "</span>\n",
    "\n",
    "We'll wrap `selenium` in a function call to make it work similarly to `requests`. Feel free to read the function comments if you want to dive deeper into `selenium`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install selenium\n",
    "%pip install webdriver-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from selenium.webdriver.edge.service import Service as EdgeService\n",
    "from selenium.webdriver.chrome.options import Options as ChromeOptions\n",
    "from selenium.webdriver.edge.options import Options as EdgeOptions\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from webdriver_manager.microsoft import EdgeChromiumDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "def get_page_content(url, browser='chrome'):\n",
    "    \"\"\"\n",
    "    Opens a URL using Selenium and retrieves the page contents\n",
    "    \n",
    "    Args:\n",
    "        url (str): The URL to open\n",
    "        browser (str): Browser to use ('chrome' or 'edge')\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (raw_html, parsed_text) where raw_html is the page source and \n",
    "               parsed_text is the cleaned text content\n",
    "    \"\"\"\n",
    "    # Set up browser options\n",
    "    if browser.lower() == 'chrome':\n",
    "        options = ChromeOptions()\n",
    "        options.add_argument('--headless')  # Run in headless mode (no GUI)\n",
    "        options.add_argument('--disable-gpu')\n",
    "        options.add_argument('--no-sandbox')\n",
    "        options.add_argument('--disable-dev-shm-usage')\n",
    "        \n",
    "        # Initialize the Chrome WebDriver\n",
    "        print('Opening Chrome Webdriver')\n",
    "        driver = webdriver.Chrome(\n",
    "            service=ChromeService(ChromeDriverManager().install()),\n",
    "            options=options\n",
    "        )\n",
    "    else:  # Edge\n",
    "        options = EdgeOptions()\n",
    "        options.add_argument('--headless')\n",
    "        options.add_argument('--disable-gpu')\n",
    "        options.add_argument('--no-sandbox')\n",
    "        options.add_argument('--disable-dev-shm-usage')\n",
    "        \n",
    "        # Initialize the Edge WebDriver\n",
    "        print('Opening Edge Webdriver')\n",
    "        driver = webdriver.Edge(\n",
    "            service=EdgeService(EdgeChromiumDriverManager().install()),\n",
    "            options=options\n",
    "        )\n",
    "    \n",
    "    print('Scrape')\n",
    "    \n",
    "    try:\n",
    "        # Open the URL\n",
    "        driver.get(url)\n",
    "        \n",
    "        # Wait for a short time to ensure the page loads\n",
    "        time.sleep(2)\n",
    "        \n",
    "        # Get the page source\n",
    "        page_content = driver.page_source\n",
    "        \n",
    "        # Parse with BeautifulSoup\n",
    "        soup = BeautifulSoup(page_content, 'html.parser')\n",
    "        \n",
    "        # Remove script and style elements\n",
    "        for script in soup([\"script\", \"style\"]):\n",
    "            script.decompose()\n",
    "        \n",
    "        # Find all code blocks and wrap their text in backticks\n",
    "        code_blocks = soup.find_all(['pre', 'code'])\n",
    "        for block in code_blocks:\n",
    "            if block.string:\n",
    "                block.string = f'```{block.string}```'\n",
    "            else:\n",
    "                # Handle nested elements within code blocks\n",
    "                block.string = f'```{block.get_text()}```'\n",
    "                \n",
    "        # Get text and clean it\n",
    "        text = soup.get_text().replace(\"```Copy\", \"```\")\n",
    "        \n",
    "        # Clean up the text\n",
    "        # Break into lines and remove leading/trailing space on each\n",
    "        lines = (line.strip() for line in text.splitlines())\n",
    "        \n",
    "        # Break multi-headlines into a line each\n",
    "        chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "        \n",
    "        # Drop blank lines\n",
    "        text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
    "        \n",
    "        return page_content, text\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        return None, None\n",
    "    \n",
    "    finally:\n",
    "        # Always close the browser\n",
    "        driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'https://msoe-maic.com/library?nav=Research'\n",
    "\n",
    "# TODO: add cases for other browsers. Chrome -> edge\n",
    "# YOU NEED Chrome or Edge installed. Sorry Mac users :(\n",
    "html, _ = get_page_content(URL, browser='edge') # Get the page html, but this time with selenium. NOTE: This will take a while to run the first time around b/c the webdriver has to be installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can now scrape the element we wanted\n",
    "\n",
    "print(BeautifulSoup(html, 'html.parser').find('div', {'id': 'agent-simulation-experts'}).prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#ff5555;font-weight:bold;font-size:1.5rem;\">\n",
    "    STOP\n",
    "</span>\n",
    "\n",
    "... or keep going if you want to work ahead.\n",
    "\n",
    "---\n",
    "\n",
    "**Using LLMs to summarize scraped data.**\n",
    "\n",
    "If you're scraping unstructured data, then LLMs are a must. Although there is structure in the HTML elements, it can often be easier to ask an LLM to structure the output for you.\n",
    "\n",
    "Let's structure the output of a page listing refurbished iPhones for sale.\n",
    "\n",
    "You will need Gemini API keys to run this example. [Link to Gemini API](https://aistudio.google.com/)\n",
    "\n",
    "<span style=\"color:#55ff55;font-weight:bold;font-size:1.5rem;\">\n",
    "    GO\n",
    "</span>\n",
    "\n",
    "This example will:\n",
    "- Use Selenium to scrape for refurbished iPhones.\n",
    "- Use an LLM to summarize the results into a structured format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pydantic\n",
      "  Using cached pydantic-2.10.6-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting pydantic-ai\n",
      "  Using cached pydantic_ai-0.0.22-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: pandas in /home/lucas/.local/lib/python3.12/site-packages (2.2.2)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.27.2 (from pydantic)\n",
      "  Using cached pydantic_core-2.27.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting typing-extensions>=4.12.2 (from pydantic)\n",
      "  Using cached typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting pydantic-ai-slim==0.0.22 (from pydantic-ai-slim[anthropic,cohere,graph,groq,mistral,openai,vertexai]==0.0.22->pydantic-ai)\n",
      "  Using cached pydantic_ai_slim-0.0.22-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting eval-type-backport>=0.2.0 (from pydantic-ai-slim==0.0.22->pydantic-ai-slim[anthropic,cohere,graph,groq,mistral,openai,vertexai]==0.0.22->pydantic-ai)\n",
      "  Using cached eval_type_backport-0.2.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting griffe>=1.3.2 (from pydantic-ai-slim==0.0.22->pydantic-ai-slim[anthropic,cohere,graph,groq,mistral,openai,vertexai]==0.0.22->pydantic-ai)\n",
      "  Using cached griffe-1.5.6-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting httpx>=0.27 (from pydantic-ai-slim==0.0.22->pydantic-ai-slim[anthropic,cohere,graph,groq,mistral,openai,vertexai]==0.0.22->pydantic-ai)\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting logfire-api>=1.2.0 (from pydantic-ai-slim==0.0.22->pydantic-ai-slim[anthropic,cohere,graph,groq,mistral,openai,vertexai]==0.0.22->pydantic-ai)\n",
      "  Using cached logfire_api-3.5.3-py3-none-any.whl.metadata (971 bytes)\n",
      "Collecting pydantic-graph==0.0.22 (from pydantic-ai-slim==0.0.22->pydantic-ai-slim[anthropic,cohere,graph,groq,mistral,openai,vertexai]==0.0.22->pydantic-ai)\n",
      "  Using cached pydantic_graph-0.0.22-py3-none-any.whl.metadata (3.9 kB)\n",
      "\u001b[33mWARNING: pydantic-ai-slim 0.0.22 does not provide the extra 'graph'\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting anthropic>=0.40.0 (from pydantic-ai-slim[anthropic,cohere,graph,groq,mistral,openai,vertexai]==0.0.22->pydantic-ai)\n",
      "  Using cached anthropic-0.45.2-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting cohere>=5.13.11 (from pydantic-ai-slim[anthropic,cohere,graph,groq,mistral,openai,vertexai]==0.0.22->pydantic-ai)\n",
      "  Using cached cohere-5.13.11-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting groq>=0.12.0 (from pydantic-ai-slim[anthropic,cohere,graph,groq,mistral,openai,vertexai]==0.0.22->pydantic-ai)\n",
      "  Using cached groq-0.18.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting mistralai>=1.2.5 (from pydantic-ai-slim[anthropic,cohere,graph,groq,mistral,openai,vertexai]==0.0.22->pydantic-ai)\n",
      "  Using cached mistralai-1.5.0-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting openai>=1.59.0 (from pydantic-ai-slim[anthropic,cohere,graph,groq,mistral,openai,vertexai]==0.0.22->pydantic-ai)\n",
      "  Using cached openai-1.61.1-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting google-auth>=2.36.0 (from pydantic-ai-slim[anthropic,cohere,graph,groq,mistral,openai,vertexai]==0.0.22->pydantic-ai)\n",
      "  Using cached google_auth-2.38.0-py2.py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting requests>=2.32.3 (from pydantic-ai-slim[anthropic,cohere,graph,groq,mistral,openai,vertexai]==0.0.22->pydantic-ai)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /home/lucas/.local/lib/python3.12/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/lucas/.local/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/lucas/.local/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/lucas/.local/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/lucas/.local/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Collecting anyio<5,>=3.5.0 (from anthropic>=0.40.0->pydantic-ai-slim[anthropic,cohere,graph,groq,mistral,openai,vertexai]==0.0.22->pydantic-ai)\n",
      "  Using cached anyio-4.8.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3.12/site-packages (from anthropic>=0.40.0->pydantic-ai-slim[anthropic,cohere,graph,groq,mistral,openai,vertexai]==0.0.22->pydantic-ai) (1.9.0)\n",
      "Collecting jiter<1,>=0.4.0 (from anthropic>=0.40.0->pydantic-ai-slim[anthropic,cohere,graph,groq,mistral,openai,vertexai]==0.0.22->pydantic-ai)\n",
      "  Using cached jiter-0.8.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: sniffio in /home/lucas/.local/lib/python3.12/site-packages (from anthropic>=0.40.0->pydantic-ai-slim[anthropic,cohere,graph,groq,mistral,openai,vertexai]==0.0.22->pydantic-ai) (1.3.1)\n",
      "Collecting fastavro<2.0.0,>=1.9.4 (from cohere>=5.13.11->pydantic-ai-slim[anthropic,cohere,graph,groq,mistral,openai,vertexai]==0.0.22->pydantic-ai)\n",
      "  Using cached fastavro-1.10.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
      "Collecting httpx-sse==0.4.0 (from cohere>=5.13.11->pydantic-ai-slim[anthropic,cohere,graph,groq,mistral,openai,vertexai]==0.0.22->pydantic-ai)\n",
      "  Using cached httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting tokenizers<1,>=0.15 (from cohere>=5.13.11->pydantic-ai-slim[anthropic,cohere,graph,groq,mistral,openai,vertexai]==0.0.22->pydantic-ai)\n",
      "  Using cached tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting types-requests<3.0.0,>=2.0.0 (from cohere>=5.13.11->pydantic-ai-slim[anthropic,cohere,graph,groq,mistral,openai,vertexai]==0.0.22->pydantic-ai)\n",
      "  Using cached types_requests-2.32.0.20241016-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth>=2.36.0->pydantic-ai-slim[anthropic,cohere,graph,groq,mistral,openai,vertexai]==0.0.22->pydantic-ai)\n",
      "  Using cached cachetools-5.5.1-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth>=2.36.0->pydantic-ai-slim[anthropic,cohere,graph,groq,mistral,openai,vertexai]==0.0.22->pydantic-ai)\n",
      "  Using cached pyasn1_modules-0.4.1-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth>=2.36.0->pydantic-ai-slim[anthropic,cohere,graph,groq,mistral,openai,vertexai]==0.0.22->pydantic-ai)\n",
      "  Using cached rsa-4.9-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting colorama>=0.4 (from griffe>=1.3.2->pydantic-ai-slim==0.0.22->pydantic-ai-slim[anthropic,cohere,graph,groq,mistral,openai,vertexai]==0.0.22->pydantic-ai)\n",
      "  Using cached colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: certifi in /home/lucas/.local/lib/python3.12/site-packages (from httpx>=0.27->pydantic-ai-slim==0.0.22->pydantic-ai-slim[anthropic,cohere,graph,groq,mistral,openai,vertexai]==0.0.22->pydantic-ai) (2024.7.4)\n",
      "Collecting httpcore==1.* (from httpx>=0.27->pydantic-ai-slim==0.0.22->pydantic-ai-slim[anthropic,cohere,graph,groq,mistral,openai,vertexai]==0.0.22->pydantic-ai)\n",
      "  Using cached httpcore-1.0.7-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: idna in /usr/lib/python3.12/site-packages (from httpx>=0.27->pydantic-ai-slim==0.0.22->pydantic-ai-slim[anthropic,cohere,graph,groq,mistral,openai,vertexai]==0.0.22->pydantic-ai) (3.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/lucas/.local/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.27->pydantic-ai-slim==0.0.22->pydantic-ai-slim[anthropic,cohere,graph,groq,mistral,openai,vertexai]==0.0.22->pydantic-ai) (0.14.0)\n",
      "Collecting jsonpath-python>=1.0.6 (from mistralai>=1.2.5->pydantic-ai-slim[anthropic,cohere,graph,groq,mistral,openai,vertexai]==0.0.22->pydantic-ai)\n",
      "  Using cached jsonpath_python-1.0.6-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting typing-inspect>=0.9.0 (from mistralai>=1.2.5->pydantic-ai-slim[anthropic,cohere,graph,groq,mistral,openai,vertexai]==0.0.22->pydantic-ai)\n",
      "  Using cached typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: tqdm>4 in /home/lucas/.local/lib/python3.12/site-packages (from openai>=1.59.0->pydantic-ai-slim[anthropic,cohere,graph,groq,mistral,openai,vertexai]==0.0.22->pydantic-ai) (4.66.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/lib/python3.12/site-packages (from requests>=2.32.3->pydantic-ai-slim[anthropic,cohere,graph,groq,mistral,openai,vertexai]==0.0.22->pydantic-ai) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3.12/site-packages (from requests>=2.32.3->pydantic-ai-slim[anthropic,cohere,graph,groq,mistral,openai,vertexai]==0.0.22->pydantic-ai) (1.26.20)\n",
      "Collecting pyasn1<0.7.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth>=2.36.0->pydantic-ai-slim[anthropic,cohere,graph,groq,mistral,openai,vertexai]==0.0.22->pydantic-ai)\n",
      "  Using cached pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.16.4 (from tokenizers<1,>=0.15->cohere>=5.13.11->pydantic-ai-slim[anthropic,cohere,graph,groq,mistral,openai,vertexai]==0.0.22->pydantic-ai)\n",
      "  Using cached huggingface_hub-0.28.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests>=2.32.3->pydantic-ai-slim[anthropic,cohere,graph,groq,mistral,openai,vertexai]==0.0.22->pydantic-ai)\n",
      "  Using cached urllib3-2.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.9.0->mistralai>=1.2.5->pydantic-ai-slim[anthropic,cohere,graph,groq,mistral,openai,vertexai]==0.0.22->pydantic-ai)\n",
      "  Using cached mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: filelock in /home/lucas/.local/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere>=5.13.11->pydantic-ai-slim[anthropic,cohere,graph,groq,mistral,openai,vertexai]==0.0.22->pydantic-ai) (3.15.4)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere>=5.13.11->pydantic-ai-slim[anthropic,cohere,graph,groq,mistral,openai,vertexai]==0.0.22->pydantic-ai)\n",
      "  Using cached fsspec-2025.2.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/lucas/.local/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere>=5.13.11->pydantic-ai-slim[anthropic,cohere,graph,groq,mistral,openai,vertexai]==0.0.22->pydantic-ai) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib64/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere>=5.13.11->pydantic-ai-slim[anthropic,cohere,graph,groq,mistral,openai,vertexai]==0.0.22->pydantic-ai) (6.0.1)\n",
      "Using cached pydantic-2.10.6-py3-none-any.whl (431 kB)\n",
      "Using cached pydantic_core-2.27.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "Using cached pydantic_ai-0.0.22-py3-none-any.whl (9.8 kB)\n",
      "Using cached pydantic_ai_slim-0.0.22-py3-none-any.whl (93 kB)\n",
      "Using cached pydantic_graph-0.0.22-py3-none-any.whl (16 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Using cached anthropic-0.45.2-py3-none-any.whl (222 kB)\n",
      "Using cached cohere-5.13.11-py3-none-any.whl (252 kB)\n",
      "Using cached httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
      "Using cached eval_type_backport-0.2.2-py3-none-any.whl (5.8 kB)\n",
      "Using cached google_auth-2.38.0-py2.py3-none-any.whl (210 kB)\n",
      "Using cached griffe-1.5.6-py3-none-any.whl (128 kB)\n",
      "Using cached groq-0.18.0-py3-none-any.whl (121 kB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Using cached httpcore-1.0.7-py3-none-any.whl (78 kB)\n",
      "Using cached logfire_api-3.5.3-py3-none-any.whl (73 kB)\n",
      "Using cached mistralai-1.5.0-py3-none-any.whl (271 kB)\n",
      "Using cached openai-1.61.1-py3-none-any.whl (463 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached anyio-4.8.0-py3-none-any.whl (96 kB)\n",
      "Using cached cachetools-5.5.1-py3-none-any.whl (9.5 kB)\n",
      "Using cached colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Downloading fastavro-1.10.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.8/3.3 MB\u001b[0m \u001b[31m8.5 kB/s\u001b[0m eta \u001b[36m0:04:56\u001b[0mm\n",
      "\u001b[?25h\u001b[31mERROR: Exception:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.12/site-packages/pip/_vendor/urllib3/response.py\", line 438, in _error_catcher\n",
      "    yield\n",
      "  File \"/usr/lib/python3.12/site-packages/pip/_vendor/urllib3/response.py\", line 561, in read\n",
      "    data = self._fp_read(amt) if not fp_closed else b\"\"\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/site-packages/pip/_vendor/urllib3/response.py\", line 527, in _fp_read\n",
      "    return self._fp.read(amt) if amt is not None else self._fp.read()\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/site-packages/pip/_vendor/cachecontrol/filewrapper.py\", line 98, in read\n",
      "    data: bytes = self.__fp.read(amt)\n",
      "                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib64/python3.12/http/client.py\", line 479, in read\n",
      "    s = self.fp.read(amt)\n",
      "        ^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib64/python3.12/socket.py\", line 720, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib64/python3.12/ssl.py\", line 1251, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib64/python3.12/ssl.py\", line 1103, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TimeoutError: The read operation timed out\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.12/site-packages/pip/_internal/cli/base_command.py\", line 180, in exc_logging_wrapper\n",
      "    status = run_func(*args)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/site-packages/pip/_internal/cli/req_command.py\", line 245, in wrapper\n",
      "    return func(self, options, args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/site-packages/pip/_internal/commands/install.py\", line 377, in run\n",
      "    requirement_set = resolver.resolve(\n",
      "                      ^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/site-packages/pip/_internal/resolution/resolvelib/resolver.py\", line 179, in resolve\n",
      "    self.factory.preparer.prepare_linked_requirements_more(reqs)\n",
      "  File \"/usr/lib/python3.12/site-packages/pip/_internal/operations/prepare.py\", line 552, in prepare_linked_requirements_more\n",
      "    self._complete_partial_requirements(\n",
      "  File \"/usr/lib/python3.12/site-packages/pip/_internal/operations/prepare.py\", line 467, in _complete_partial_requirements\n",
      "    for link, (filepath, _) in batch_download:\n",
      "                               ^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/site-packages/pip/_internal/network/download.py\", line 183, in __call__\n",
      "    for chunk in chunks:\n",
      "                 ^^^^^^\n",
      "  File \"/usr/lib/python3.12/site-packages/pip/_internal/cli/progress_bars.py\", line 53, in _rich_progress_bar\n",
      "    for chunk in iterable:\n",
      "                 ^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/site-packages/pip/_internal/network/utils.py\", line 63, in response_chunks\n",
      "    for chunk in response.raw.stream(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/site-packages/pip/_vendor/urllib3/response.py\", line 622, in stream\n",
      "    data = self.read(amt=amt, decode_content=decode_content)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/site-packages/pip/_vendor/urllib3/response.py\", line 560, in read\n",
      "    with self._error_catcher():\n",
      "         ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib64/python3.12/contextlib.py\", line 158, in __exit__\n",
      "    self.gen.throw(value)\n",
      "  File \"/usr/lib/python3.12/site-packages/pip/_vendor/urllib3/response.py\", line 443, in _error_catcher\n",
      "    raise ReadTimeoutError(self._pool, None, \"Read timed out.\")\n",
      "pip._vendor.urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='files.pythonhosted.org', port=443): Read timed out.\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pydantic pydantic-ai pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pydantic_ai import Agent\n",
    "from typing import List\n",
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'https://www.apple.com/shop/refurbished/iphone/iphone-14-pro'\n",
    "\n",
    "# --- NOTE: put your key here ---\n",
    "os.environ[\"GEMINI_API_KEY\"]  = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is where you can specify the output structure\n",
    "\n",
    "class ProductResult(BaseModel):  \n",
    "    model: str = Field(description='The model of the product')\n",
    "    description: str = Field(description='The description of the product')\n",
    "    cost: int = Field(description=\"The cost of the product\")\n",
    "    isp: str = Field(description=\"The internet service provider\")\n",
    "    color: str = Field(description=\"The color of the product\")\n",
    "    refurbished: bool = Field(description=\"Whether the product is refurbished\")\n",
    "\n",
    "class RequestResults(BaseModel):\n",
    "    products: List[ProductResult] = Field(description='The list of product results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent( # Create an agent that will structure the output\n",
    "    'google-gla:gemini-1.5-flash',\n",
    "    result_type=RequestResults,\n",
    "    system_prompt='Be concise, reply with one sentence.',  \n",
    ")\n",
    "\n",
    "# Agent system prompt - tell it what to do\n",
    "@agent.system_prompt  \n",
    "async def add_customer_name(ctx) -> str:\n",
    "    return f\"Your goal is to extract product information from web scraped pages and format it to a structured response.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, text = get_page_content(URL, browser='edge') # Scrape the list of iPhones, and get the text (so the LLM can read it more easily)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert os.environ[\"GEMINI_API_KEY\"] # You need to specify your gemini key above!\n",
    "\n",
    "result = await agent.run(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "item_dicts = [item.dict() for item in result.data.products]\n",
    "df = pd.DataFrame(item_dicts)\n",
    "display(df.head(20))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
