{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c19f5c6",
   "metadata": {},
   "source": [
    "# Using Rosie's NVIDIA NIM for LLMs\n",
    "\n",
    "Rosie has an always available instance of [NVIDIA NIM for LLMs](https://docs.nvidia.com/nim/large-language-models/latest/introduction.html) running [Llama-3.3-70B-Instruct](https://catalog.ngc.nvidia.com/orgs/nim/teams/meta/models/llama-3.3-70b-instruct). You can access it from code running on Rosie."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76edc4b4",
   "metadata": {},
   "source": [
    "NIM for LLMs works by implementing the OpenAI API. Thus, to talk to it from Python, we use the `openai` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AsyncOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a67f16",
   "metadata": {},
   "source": [
    "We then instantiate an instance of this class, which lets us talk with the services hosted on NIM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9120356",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = AsyncOpenAI(\n",
    "   base_url = \"http://dh-dgxh100-2.hpc.msoe.edu:8000/v1\",\n",
    "   api_key = \"not_used\" # this field needs to be included but is ignored\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff1681b",
   "metadata": {},
   "source": [
    "We can list out the currently available models by calling `client.models.list()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017e50ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "async for model in client.models.list():\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59095e0",
   "metadata": {},
   "source": [
    "We use the completions API to prompt the model for a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b44306",
   "metadata": {},
   "outputs": [],
   "source": [
    "await client.completions.create(\n",
    "   model=\"meta/llama-3.3-70b-instruct\",\n",
    "   prompt=\"Why is MSOE the best school to study CS?\", # your prompt goes here\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0771ad6e",
   "metadata": {},
   "source": [
    "If we want to get back the response token by token as it is generated, we can use the `stream` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfe4e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "async for event in await client.completions.create(\n",
    "        model=\"meta/llama-3.3-70b-instruct\",\n",
    "        prompt=\"Why is MSOE the best school to study CS?\",\n",
    "        stream=True,\n",
    "    ):\n",
    "    print(event)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e78697",
   "metadata": {},
   "source": [
    "We can control how long the response is by setting the `max_tokens` parameter. This is a tradeoff between the amount of information we get back and the time it takes to get a response. The more tokens we ask for, the longer it will take to get a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b5a5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "await client.completions.create(\n",
    "    model=\"meta/llama-3.3-70b-instruct\",\n",
    "    prompt=\"Why is MSOE the best school to study CS?\",\n",
    "    max_tokens=200,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30e8f44",
   "metadata": {},
   "source": [
    "We can control how repetitive the model allows itself to be by setting the `frequency_penalty` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097f37b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(await client.completions.create(\n",
    "   model=\"meta/llama-3.3-70b-instruct\",\n",
    "   prompt=\"Repeat the word poem.\",\n",
    "   frequency_penalty=-2,\n",
    "))\n",
    "print(await client.completions.create(\n",
    "   model=\"meta/llama-3.3-70b-instruct\",\n",
    "   prompt=\"Repeat the word poem.\",\n",
    "   frequency_penalty=2,\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0306e3e8",
   "metadata": {},
   "source": [
    "If you need the prompt you gave the model back in the response, you can use the `echo` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3c5dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "await client.completions.create(\n",
    "    model=\"meta/llama-3.3-70b-instruct\",\n",
    "    prompt=\"Why is MSOE the best school to study CS?\",\n",
    "    echo=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fe569d",
   "metadata": {},
   "source": [
    "You can force the model to talk about new topics by increasing the `presence_penalty` parameter. However, setting it too high will likely cause the model to veer further off topic than you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165fdbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "await client.completions.create(\n",
    "    model=\"meta/llama-3.3-70b-instruct\",\n",
    "    prompt=\"Why is MSOE the best school to study CS?\",\n",
    "    presence_penalty=2,\n",
    "    max_tokens=100\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f9c807",
   "metadata": {},
   "source": [
    "The completions API is for cases where you just want to predict next words instead of having a conversation. If you do want to chat with the model, you can instead use the chat completion API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9429de35",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Hello! How are you?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Hi! I am quite well, how can I help you today?\"},\n",
    "    {\"role\": \"user\", \"content\": \"Why is MSOE the best school to study CS?\"}\n",
    "]\n",
    "await client.chat.completions.create(\n",
    "    model=\"meta/llama-3.3-70b-instruct\",\n",
    "    messages=messages,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51f6d1a",
   "metadata": {},
   "source": [
    "Like the completions API, the chat completion API has a `stream` parameter that lets you get back the response token by token as it is generated. It also has a `max_tokens` parameter that lets you control how long the response is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00e6cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "async for token in await client.chat.completions.create(\n",
    "        model=\"meta/llama-3.3-70b-instruct\",\n",
    "        messages=messages,\n",
    "        stream=True,\n",
    "        max_tokens=10,\n",
    "    ):\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c99084a",
   "metadata": {},
   "source": [
    "`frequency_penalty` is also available in the chat completion API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4dfe9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "poem_message = [\n",
    "    {\"role\": \"user\", \"content\": \"Continuously repeat the word poem.\"},\n",
    "]\n",
    "\n",
    "\n",
    "print(await client.chat.completions.create(\n",
    "   model=\"meta/llama-3.3-70b-instruct\",\n",
    "   messages=poem_message,\n",
    "   frequency_penalty=-2,\n",
    "   max_tokens=30,\n",
    "))\n",
    "print(await client.chat.completions.create(\n",
    "   model=\"meta/llama-3.3-70b-instruct\",\n",
    "   messages=poem_message,\n",
    "   frequency_penalty=2,\n",
    "   max_tokens=30,\n",
    "))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
