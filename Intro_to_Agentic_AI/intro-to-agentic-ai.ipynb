{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a26705d6",
      "metadata": {},
      "source": [
        "# Intro to Agentic AI\n",
        "\n",
        "MSOE AI Club Workshop\n",
        "```\n",
        "  _____________\n",
        " /0   /     \\  \\\n",
        "/  \\ M A I C/  /\\\n",
        "\\ / *      /  / /\n",
        " \\___\\____/  @ /\n",
        "          \\_/_/\n",
        "```\n",
        "\n",
        "*(ROSIE is not needed for this workshop!)*\n",
        "\n",
        "Prereqs:\n",
        "- Install [VSCode](https://code.visualstudio.com/)\n",
        "- Install [Python](https://www.python.org/downloads/)\n",
        "- Ensure you can run notebooks in VSCode.\n",
        "- **For MCP servers (optional):** Install [Node.js](https://nodejs.org/) so you can run servers like `npx -y @modelcontextprotocol/server-filesystem` or `server-github`.\n",
        "\n",
        "Run the below pip installs now so we don't have to wait for them later: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7978eff-7664-49e8-87ab-bf93e3fa9112",
      "metadata": {},
      "outputs": [],
      "source": [
        "# %pip install langchain langchain-core langchain-community langgraph langchain-google-genai langchain-openai tavily-python mcp nest_asyncio pillow python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cac37e0a-ea04-4a31-9ae9-6f59ca88c6db",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import json\n",
        "import sqlite3\n",
        "import textwrap\n",
        "import base64\n",
        "import traceback\n",
        "import asyncio\n",
        "import nest_asyncio\n",
        "import subprocess\n",
        "import threading\n",
        "from datetime import date\n",
        "from typing import Any, Dict, List, Optional, TypedDict\n",
        "from pathlib import Path\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "from langchain_core.tools import tool\n",
        "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage, ToolMessage\n",
        "from langchain_core.language_models.chat_models import BaseChatModel\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langgraph.graph import StateGraph, END\n",
        "from tavily import TavilyClient\n",
        "from mcp import ClientSession, StdioServerParameters\n",
        "from mcp.client.stdio import stdio_client\n",
        "from langchain_openai import ChatOpenAI"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0e0311f",
      "metadata": {},
      "source": [
        "<span style=\"color:#ff5555;font-weight:bold;font-size:1.5rem;\">\n",
        "    STOP\n",
        "</span>\n",
        "\n",
        "... or keep going if you want to work ahead.\n",
        "\n",
        "---\n",
        "\n",
        "**What is Agentic AI?**\n",
        "\n",
        "Unlike a simple chatbot that only generates one reply per turn, an *agent* can **use tools** and work **autonomously**: search the web, run code, read/write files, call APIs, and remember facts across conversations. The model decides *when* to call which tool and then continues reasoning with the results.\n",
        "\n",
        "In this workshop we'll build an agent that has:\n",
        "- **Web search** (e.g. Tavily) for up-to-date information\n",
        "- **A Python interpreter** for math, data, and file operations\n",
        "- **Persistent memory** so it can remember your name, preferences, and project details\n",
        "- **MCP (Model Context Protocol)** so it can use external tools (filesystem, GitHub, etc.) when you connect them\n",
        "- **Multimodal input** so you can send images (e.g. photos of equipment) and get descriptions or advice\n",
        "\n",
        "We'll use **LangGraph** to define the flow: *main agent ‚Üí optional tool calls (looping) ‚Üí end*, with a **memory manager** running in the background after each turn so it never slows you down. Run the pip installs above, then we'll start with imports and environment setup.\n",
        "\n",
        "---\n",
        "\n",
        "<span style=\"color:#55ff55;font-weight:bold;font-size:1.5rem;\">\n",
        "    GO\n",
        "</span>\n",
        "\n",
        "Run the cell below to add your API keys to your environment variables\n",
        "> NOTE: If you are running this notebook or doing this workshop after the February 12th, 2026 event, you will need to obtain your own API keys as we have rotated them. Here are the sites you need keys from, you can get a free tier for each of these:\n",
        "> - https://aistudio.google.com/\n",
        "> - https://www.tavily.com/\n",
        "> \n",
        "> Optionally, you can instead obtain an OpenAI, they just do not have a free tier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8edefce-3796-4830-b3c3-9d3049d31ef2",
      "metadata": {},
      "outputs": [],
      "source": [
        "load_dotenv(override=False)  # won't overwrite vars already set in os.environ\n",
        "\n",
        "# You can still set keys manually here ‚Äî these take priority over the .env file:\n",
        "os.environ[\"TAVILY_API_KEY\"] = ...              # Add a Tavily key here for the workshop\n",
        "# os.environ[\"OPENAI_API_KEY\"] = \"sk-proj-...\"  # Uncomment to use OpenAI instead of Gemini\n",
        "# os.environ[\"GEMINI_API_KEY\"] = \"AI...\"        # Uncomment to use Gemini instead of OpenAI\n",
        "\n",
        "# Quick sanity check ‚Äî show which keys are available\n",
        "for _key in (\"OPENAI_API_KEY\", \"TAVILY_API_KEY\", \"GEMINI_API_KEY\"):\n",
        "    _val = os.environ.get(_key)\n",
        "    if _val:\n",
        "        print(f\"  {_key} = {_val[:4]}...{_val[-4:]}\")\n",
        "    else:\n",
        "        print(f\"  {_key} = [NOT SET]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1505047a",
      "metadata": {},
      "source": [
        "<span style=\"color:#ff5555;font-weight:bold;font-size:1.5rem;\">\n",
        "    STOP\n",
        "</span>\n",
        "\n",
        "... or keep going if you want to work ahead.\n",
        "\n",
        "---\n",
        "\n",
        "**Why do we need a memory store?**\n",
        "\n",
        "Agents often need to remember things *across* sessions: your name, timezone, project path, or ‚Äúalways use Python 3.11.‚Äù If we only kept the current chat history, that context would be lost when the conversation gets long or when you start a new session.\n",
        "\n",
        "We‚Äôll use a small **persistent store** (SQLite) with:\n",
        "- **Upsert** (create/update) by key\n",
        "- **Search** by query text (for retrieval when answering)\n",
        "- **List** recent memories\n",
        "- **Delete** when something is wrong or outdated\n",
        "\n",
        "A separate **memory manager** step (later) will decide *what* to store or remove based on the conversation, so the main agent doesn‚Äôt have to worry about that.\n",
        "\n",
        "---\n",
        "\n",
        "<span style=\"color:#55ff55;font-weight:bold;font-size:1.5rem;\">\n",
        "    GO\n",
        "</span>\n",
        "\n",
        "The next cell defines `MemoryStore` and a global `MEM` instance. Run it to create the DB and have `MEM` ready for the memory tools."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b1b8bbb-eb44-488e-9469-1e2ce070afb7",
      "metadata": {},
      "outputs": [],
      "source": [
        "def normalize_ai_content(content) -> str:\n",
        "    if isinstance(content, str):\n",
        "        return content\n",
        "\n",
        "    if isinstance(content, list):\n",
        "        texts = []\n",
        "        for part in content:\n",
        "            if isinstance(part, dict):\n",
        "                if part.get(\"type\") == \"text\" and \"text\" in part:\n",
        "                    texts.append(part[\"text\"])\n",
        "                elif \"text\" in part:\n",
        "                    texts.append(str(part[\"text\"]))\n",
        "            elif isinstance(part, str):\n",
        "                texts.append(part)\n",
        "        return \"\\n\".join(t for t in texts if t).strip()\n",
        "\n",
        "    return str(content)\n",
        "\n",
        "class MemoryStore:\n",
        "    \"\"\"Simple persistent key/value memory with tags + freeform text.\"\"\"\n",
        "    def __init__(self, path: str = \"agent_memory.sqlite3\"):\n",
        "        self.path = path\n",
        "        self._init_db()\n",
        "\n",
        "    def _init_db(self):\n",
        "        with sqlite3.connect(self.path) as con:\n",
        "            con.execute(\"\"\"\n",
        "                CREATE TABLE IF NOT EXISTS memories (\n",
        "                    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "                    key TEXT UNIQUE,\n",
        "                    value TEXT NOT NULL,\n",
        "                    tags TEXT DEFAULT '[]',\n",
        "                    updated_at DATETIME DEFAULT CURRENT_TIMESTAMP\n",
        "                )\n",
        "            \"\"\")\n",
        "            con.commit()\n",
        "\n",
        "    def upsert(self, key: str, value: str, tags: Optional[List[str]] = None) -> str:\n",
        "        tags_json = json.dumps(tags or [])\n",
        "        with sqlite3.connect(self.path) as con:\n",
        "            con.execute(\"\"\"\n",
        "                INSERT INTO memories(key, value, tags, updated_at)\n",
        "                VALUES(?, ?, ?, CURRENT_TIMESTAMP)\n",
        "                ON CONFLICT(key) DO UPDATE SET\n",
        "                    value=excluded.value,\n",
        "                    tags=excluded.tags,\n",
        "                    updated_at=CURRENT_TIMESTAMP\n",
        "            \"\"\", (key, value, tags_json))\n",
        "            con.commit()\n",
        "        return f\"Saved memory: {key}\"\n",
        "\n",
        "    def delete(self, key: str) -> str:\n",
        "        with sqlite3.connect(self.path) as con:\n",
        "            cur = con.execute(\"DELETE FROM memories WHERE key=?\", (key,))\n",
        "            con.commit()\n",
        "        return f\"Deleted memory: {key}\" if cur.rowcount else f\"No memory found for key: {key}\"\n",
        "\n",
        "    def list_all(self, limit: int = 50) -> List[Dict[str, Any]]:\n",
        "        with sqlite3.connect(self.path) as con:\n",
        "            rows = con.execute(\"\"\"\n",
        "                SELECT key, value, tags, updated_at\n",
        "                FROM memories\n",
        "                ORDER BY updated_at DESC\n",
        "                LIMIT ?\n",
        "            \"\"\", (limit,)).fetchall()\n",
        "        return [{\"key\": k, \"value\": v, \"tags\": json.loads(t), \"updated_at\": u} for k, v, t, u in rows]\n",
        "\n",
        "    def search(self, query: str, limit: int = 10) -> List[Dict[str, Any]]:\n",
        "        q = f\"%{query.lower()}%\"\n",
        "        with sqlite3.connect(self.path) as con:\n",
        "            rows = con.execute(\"\"\"\n",
        "                SELECT key, value, tags, updated_at\n",
        "                FROM memories\n",
        "                WHERE lower(key) LIKE ? OR lower(value) LIKE ?\n",
        "                ORDER BY updated_at DESC\n",
        "                LIMIT ?\n",
        "            \"\"\", (q, q, limit)).fetchall()\n",
        "        return [{\"key\": k, \"value\": v, \"tags\": json.loads(t), \"updated_at\": u} for k, v, t, u in rows]\n",
        "\n",
        "\n",
        "MEM = MemoryStore(path=\"agent_memory.sqlite3\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef35070a",
      "metadata": {},
      "source": [
        "<span style=\"color:#ff5555;font-weight:bold;font-size:1.5rem;\">\n",
        "    STOP\n",
        "</span>\n",
        "\n",
        "... or keep going if you want to work ahead.\n",
        "\n",
        "---\n",
        "\n",
        "**What is MCP (Model Context Protocol)?**\n",
        "\n",
        "MCP lets your agent use **external servers** that expose tools over a standard protocol. It's like a programmatic way to allow your agent take actions! For example:\n",
        "- **Filesystem server** ‚Äì read/write files in a directory\n",
        "- **GitHub server** ‚Äì list repos, create issues, read files (with a token)\n",
        "- **Custom servers** ‚Äì your own tools (databases, APIs, etc.)\n",
        "\n",
        "The notebook will **connect** to a server by running its command (e.g. `npx -y @modelcontextprotocol/server-filesystem /path`). Once connected, the agent gets a list of tools and can call them via a wrapper tool we‚Äôll define. All of this is async, so we use a helper to run it from the notebook.\n",
        "\n",
        "---\n",
        "\n",
        "<span style=\"color:#55ff55;font-weight:bold;font-size:1.5rem;\">\n",
        "    GO\n",
        "</span>\n",
        "\n",
        "The next cell defines `MCPManager`: connect, disconnect, call a tool, list tools. The cell after that creates the global `MCP` and a `run_async` helper. Run both."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b206099",
      "metadata": {},
      "outputs": [],
      "source": [
        "class MCPManager:\n",
        "    \"\"\"Manages connections to MCP servers and exposes their tools.\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.servers: Dict[str, dict] = {}  # name -> {process, session, tools, read, write}\n",
        "    \n",
        "    async def connect(self, name: str, command: str, args: List[str] = None, env: Dict[str, str] = None) -> str:\n",
        "        \"\"\"Connect to an MCP server via stdio.\"\"\"\n",
        "        if name in self.servers:\n",
        "            return f\"Already connected to '{name}'\"\n",
        "        \n",
        "        print(f\"üîå [MCP] Connecting to server: {name}\")\n",
        "        print(f\"   Command: {command} {' '.join(args or [])}\")\n",
        "        \n",
        "        full_env = os.environ.copy()\n",
        "        if env:\n",
        "            full_env.update(env)\n",
        "        \n",
        "        if 'github' in name.lower() and 'GITHUB_TOKEN' not in full_env:\n",
        "            print(\"   ‚ö†Ô∏è  Warning: GITHUB_TOKEN not set. GitHub server may fail.\")\n",
        "            print(\"   Set with: os.environ['GITHUB_TOKEN'] = 'your-token'\")\n",
        "        \n",
        "        try:\n",
        "            server_params = StdioServerParameters(\n",
        "                command=command,\n",
        "                args=args or [],\n",
        "                env=full_env\n",
        "            )\n",
        "            \n",
        "            client_cm = stdio_client(server_params)\n",
        "            read_stream, write_stream = await asyncio.wait_for(\n",
        "                client_cm.__aenter__(),\n",
        "                timeout=30\n",
        "            )\n",
        "            \n",
        "            session = ClientSession(read_stream, write_stream)\n",
        "            await session.__aenter__()\n",
        "            \n",
        "            init_result = await asyncio.wait_for(\n",
        "                session.initialize(),\n",
        "                timeout=30\n",
        "            )\n",
        "            \n",
        "            tools_result = await session.list_tools()\n",
        "            tools = tools_result.tools if hasattr(tools_result, 'tools') else []\n",
        "            \n",
        "            self.servers[name] = {\n",
        "                'session': session,\n",
        "                'client_cm': client_cm,\n",
        "                'tools': tools,\n",
        "                'command': command,\n",
        "                'args': args or []\n",
        "            }\n",
        "            \n",
        "            tool_names = [t.name for t in tools]\n",
        "            print(f\"   ‚úÖ Connected! Available tools ({len(tool_names)}):\")\n",
        "            for t in tools:\n",
        "                desc = getattr(t, 'description', '')[:60]\n",
        "                print(f\"      ‚Ä¢ {t.name}: {desc}...\")\n",
        "            \n",
        "            return f\"Connected to '{name}' with {len(tool_names)} tools\"\n",
        "            \n",
        "        except asyncio.TimeoutError:\n",
        "            print(f\"   ‚ùå Connection timed out\")\n",
        "            return f\"Failed to connect to '{name}': timeout\"\n",
        "        except FileNotFoundError as e:\n",
        "            print(f\"   ‚ùå Command not found: {command}\")\n",
        "            print(f\"   Make sure npx/node is installed and in PATH\")\n",
        "            return f\"Failed to connect to '{name}': command not found\"\n",
        "        except Exception as e:\n",
        "            print(f\"   ‚ùå Connection failed: {type(e).__name__}: {e}\")\n",
        "            traceback.print_exc()\n",
        "            return f\"Failed to connect to '{name}': {e}\"\n",
        "    \n",
        "    async def disconnect(self, name: str) -> str:\n",
        "        \"\"\"Disconnect from an MCP server.\"\"\"\n",
        "        if name not in self.servers:\n",
        "            return f\"Not connected to '{name}'\"\n",
        "        \n",
        "        print(f\"üîå [MCP] Disconnecting from: {name}\")\n",
        "        \n",
        "        try:\n",
        "            info = self.servers[name]\n",
        "            await info['session'].__aexit__(None, None, None)\n",
        "            await info['client_cm'].__aexit__(None, None, None)\n",
        "            del self.servers[name]\n",
        "            print(f\"   ‚úÖ Disconnected\")\n",
        "            return f\"Disconnected from '{name}'\"\n",
        "        except Exception as e:\n",
        "            print(f\"   ‚ùå Error: {e}\")\n",
        "\n",
        "            if name in self.servers:\n",
        "                del self.servers[name]\n",
        "            return f\"Disconnected from '{name}' (with errors: {e})\"\n",
        "    \n",
        "    async def call_tool(self, server_name: str, tool_name: str, arguments: dict) -> str:\n",
        "        \"\"\"Call a tool on an MCP server.\"\"\"\n",
        "        if server_name not in self.servers:\n",
        "            return f\"Not connected to server '{server_name}'. Use /mcp list to see connected servers.\"\n",
        "        \n",
        "        session = self.servers[server_name]['session']\n",
        "        \n",
        "        print(f\"üîß [MCP:{server_name}] Calling tool: {tool_name}\")\n",
        "        if arguments:\n",
        "            print(f\"   Arguments: {json.dumps(arguments, indent=2)}\")\n",
        "        \n",
        "        try:\n",
        "            result = await asyncio.wait_for(\n",
        "                session.call_tool(tool_name, arguments),\n",
        "                timeout=60\n",
        "            )\n",
        "            \n",
        "            output = \"\"\n",
        "            if hasattr(result, 'content'):\n",
        "                for item in result.content:\n",
        "                    if hasattr(item, 'text'):\n",
        "                        output += item.text + \"\\n\"\n",
        "                    elif hasattr(item, 'data'):\n",
        "                        output += f\"[Binary data: {len(item.data)} bytes]\\n\"\n",
        "            \n",
        "            output = output.strip() or str(result)\n",
        "            \n",
        "            display = output[:500] + '...' if len(output) > 500 else output\n",
        "            print(f\"   ‚úÖ Result:\\n{display}\")\n",
        "            \n",
        "            return output\n",
        "            \n",
        "        except asyncio.TimeoutError:\n",
        "            print(f\"   ‚ùå Tool call timed out\")\n",
        "            return f\"Tool call timed out after 60 seconds\"\n",
        "        except Exception as e:\n",
        "            print(f\"   ‚ùå Error: {type(e).__name__}: {e}\")\n",
        "            return f\"Error calling tool: {e}\"\n",
        "    \n",
        "    def list_servers(self) -> str:\n",
        "        \"\"\"List connected MCP servers and their tools.\"\"\"\n",
        "        if not self.servers:\n",
        "            return \"No MCP servers connected.\\n\\nUse: /mcp connect <name> <command> [args...]\"\n",
        "        \n",
        "        lines = [\"Connected MCP servers:\"]\n",
        "        for name, info in self.servers.items():\n",
        "            tool_names = [t.name for t in info['tools']]\n",
        "            lines.append(f\"\\n  üì° {name} ({len(tool_names)} tools)\")\n",
        "            for tn in tool_names[:10]:\n",
        "                lines.append(f\"     ‚Ä¢ {tn}\")\n",
        "            if len(tool_names) > 10:\n",
        "                lines.append(f\"     ... and {len(tool_names) - 10} more\")\n",
        "        \n",
        "        return \"\\n\".join(lines)\n",
        "    \n",
        "    def get_all_tools_info(self) -> List[dict]:\n",
        "        \"\"\"Get info about all tools from all connected servers.\"\"\"\n",
        "        all_tools = []\n",
        "        for server_name, info in self.servers.items():\n",
        "            for tool in info['tools']:\n",
        "                all_tools.append({\n",
        "                    'server': server_name,\n",
        "                    'name': tool.name,\n",
        "                    'description': getattr(tool, 'description', ''),\n",
        "                    'input_schema': getattr(tool, 'inputSchema', {})\n",
        "                })\n",
        "        return all_tools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bbf279bf",
      "metadata": {},
      "outputs": [],
      "source": [
        "MCP = MCPManager()\n",
        "\n",
        "def run_async(coro):\n",
        "    \"\"\"Helper to run async code in sync context.\"\"\"\n",
        "    try:\n",
        "        nest_asyncio.apply()\n",
        "    except ImportError:\n",
        "        pass\n",
        "    \n",
        "    try:\n",
        "        loop = asyncio.get_event_loop()\n",
        "    except RuntimeError:\n",
        "        loop = asyncio.new_event_loop()\n",
        "        asyncio.set_event_loop(loop)\n",
        "    \n",
        "    return loop.run_until_complete(coro)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d98235d4",
      "metadata": {},
      "source": [
        "<span style=\"color:#ff5555;font-weight:bold;font-size:1.5rem;\">\n",
        "    STOP\n",
        "</span>\n",
        "\n",
        "... or keep going if you want to work ahead.\n",
        "\n",
        "---\n",
        "\n",
        "**Multimodal input: images**\n",
        "\n",
        "Many models (including OpenAI and Gemini) can take **images** with the user message. That lets the agent answer questions about screenshots, diagrams, or photos (e.g. ‚ÄúWhat‚Äôs wrong with this water heater?‚Äù). We‚Äôll load images from disk, encode them as base64 (a format that allows us convert files into text that we can more easily send over the Internet), and attach them to a `HumanMessage` in the format the model expects. The chat loop will support an `/img` command to queue images for the next message.\n",
        "\n",
        "---\n",
        "\n",
        "<span style=\"color:#55ff55;font-weight:bold;font-size:1.5rem;\">\n",
        "    GO\n",
        "</span>\n",
        "\n",
        "The next cells define `load_image_as_base64` and `create_multimodal_message`. Run them so the chat loop can attach images to user messages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e395ac69",
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_image_as_base64(path: str) -> dict:\n",
        "    \"\"\"Load an image file and return it as a base64-encoded data dict for the model.\"\"\"\n",
        "    path = Path(path).expanduser().resolve()\n",
        "    \n",
        "    if not path.exists():\n",
        "        raise FileNotFoundError(f\"Image not found: {path}\")\n",
        "    \n",
        "    suffix = path.suffix.lower()\n",
        "    mime_types = {\n",
        "        '.jpg': 'image/jpeg',\n",
        "        '.jpeg': 'image/jpeg',\n",
        "        '.png': 'image/png',\n",
        "        '.gif': 'image/gif',\n",
        "        '.webp': 'image/webp',\n",
        "    }\n",
        "    mime_type = mime_types.get(suffix)\n",
        "    if not mime_type:\n",
        "        raise ValueError(f\"Unsupported image format: {suffix}. Use jpg, png, gif, or webp.\")\n",
        "    \n",
        "    with open(path, 'rb') as f:\n",
        "        image_data = base64.standard_b64encode(f.read()).decode('utf-8')\n",
        "    \n",
        "    return {\n",
        "        \"type\": \"image_url\",\n",
        "        \"image_url\": {\"url\": f\"data:{mime_type};base64,{image_data}\"}\n",
        "    }\n",
        "\n",
        "def create_multimodal_message(text: str, image_paths: List[str] = None) -> HumanMessage:\n",
        "    \"\"\"Create a HumanMessage with text and optional images.\"\"\"\n",
        "    if not image_paths:\n",
        "        return HumanMessage(content=text)\n",
        "    \n",
        "    content = []\n",
        "    \n",
        "    for img_path in image_paths:\n",
        "        try:\n",
        "            img_data = load_image_as_base64(img_path)\n",
        "            content.append(img_data)\n",
        "            print(f\"üì∑ Attached image: {img_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è  Failed to load image {img_path}: {e}\")\n",
        "    \n",
        "    # Add text\n",
        "    content.append({\"type\": \"text\", \"text\": text})\n",
        "    \n",
        "    return HumanMessage(content=content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "126c442c",
      "metadata": {},
      "source": [
        "<span style=\"color:#ff5555;font-weight:bold;font-size:1.5rem;\">\n",
        "    STOP\n",
        "</span>\n",
        "\n",
        "... or keep going if you want to work ahead.\n",
        "\n",
        "---\n",
        "\n",
        "**Tools: web search**\n",
        "\n",
        "LLMs are trained on a fixed snapshot of the world. To get **current** information (news, docs, prices, ‚Äúwhat‚Äôs the latest ‚Ä¶‚Äù), the agent needs to call a search API. We‚Äôll use **Tavily** (or you can swap in another provider). The tool is a single function the model can invoke with a query; it returns snippets the agent can cite. Set `TAVILY_API_KEY` in your environment (or in the keys cell) for this to work.\n",
        "\n",
        "---\n",
        "\n",
        "<span style=\"color:#55ff55;font-weight:bold;font-size:1.5rem;\">\n",
        "    GO\n",
        "</span>\n",
        "\n",
        "The next cell defines `web_search_impl` and the `@tool` `web_search`. Run it, then we‚Äôll add the Python interpreter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5618a8cd-371b-4274-bec9-2c504fcd61f2",
      "metadata": {},
      "outputs": [],
      "source": [
        "def web_search_impl(query: str, k: int = 5) -> str:\n",
        "    \"\"\"\n",
        "    Plug in any search provider.\n",
        "    Recommended: Tavily (reliable) or Serper.\n",
        "    \"\"\"\n",
        "    print(f\"üîç [Web Search] Searching for: \\\"{query}\\\"\")\n",
        "    \n",
        "    tavily_key = os.getenv(\"TAVILY_API_KEY\")\n",
        "    if not tavily_key:\n",
        "        print(\"   ‚ùå Web search not configured\")\n",
        "        return (\n",
        "            \"Web search is not configured. Set TAVILY_API_KEY to enable search.\\n\"\n",
        "            \"Query was: \" + query\n",
        "        )\n",
        "    \n",
        "    client = TavilyClient(api_key=tavily_key)\n",
        "    res = client.search(query=query, max_results=k)\n",
        "    \n",
        "    chunks = []\n",
        "    for r in res.get(\"results\", []):\n",
        "        chunks.append(f\"- {r.get('title','(no title)')}\\n  {r.get('url')}\\n  {r.get('content','')[:400]}\")\n",
        "    \n",
        "    result_count = len(chunks)\n",
        "    print(f\"   ‚úÖ Found {result_count} results\")\n",
        "    \n",
        "    return \"\\n\".join(chunks) if chunks else \"No results.\"\n",
        "\n",
        "@tool\n",
        "def web_search(query: str, k: int = 5) -> str:\n",
        "    \"\"\"Search the web for up-to-date information. Returns top results with snippets.\"\"\"\n",
        "    return web_search_impl(query=query, k=k)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98380671",
      "metadata": {},
      "source": [
        "<span style=\"color:#ff5555;font-weight:bold;font-size:1.5rem;\">\n",
        "    STOP\n",
        "</span>\n",
        "\n",
        "... or keep going if you want to work ahead.\n",
        "\n",
        "---\n",
        "\n",
        "**Tools: Python interpreter**\n",
        "\n",
        "Giving the agent the ability to **run Python** lets it do **deterministic** math, parse data, transform files, and save outputs. This means that instead of the agent hallucinating information that might be true or false, it can run code to **confirm** it. Because this runs real code on your machine, the notebook implementation **asks for confirmation** before executing. In production you‚Äôd add sandboxing, timeouts, and stricter limits. For the workshop, always review the code the agent wants to run and only confirm when you‚Äôre comfortable.\n",
        "\n",
        "---\n",
        "\n",
        "<span style=\"color:#55ff55;font-weight:bold;font-size:1.5rem;\">\n",
        "    GO\n",
        "</span>\n",
        "\n",
        "The next cell defines the `python_interpreter` tool (with a confirmation prompt). Run it, then we‚Äôll add memory and MCP tools."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b87236bc-d9ef-4db6-a102-aa91f8317df5",
      "metadata": {},
      "outputs": [],
      "source": [
        "@tool\n",
        "def python_interpreter(code: str) -> str:\n",
        "    \"\"\"\n",
        "    Execute arbitrary Python code. Returns stdout output or the result.\n",
        "    WARNING: This executes code without restrictions - use with caution.\n",
        "    You can save files using standard Python (open(), pathlib, etc.).\n",
        "    For organization, consider saving to a subfolder like 'outputs/' or 'generated/'.\n",
        "    \"\"\"\n",
        "    \n",
        "    cwd = os.getcwd()\n",
        "    \n",
        "    print(f\"üêç [Python] Agent wants to execute:\")\n",
        "    print(f\"   üìÅ Working directory: {cwd}\")\n",
        "    print(\"‚îÄ\" * 40)\n",
        "    print(code)\n",
        "    print(\"‚îÄ\" * 40)\n",
        "    \n",
        "    confirm = input(\"Execute this code? (y/n): \").strip().lower()\n",
        "    if confirm not in ('y', 'yes'):\n",
        "        print(\"   ‚õî Execution cancelled by user\")\n",
        "        return \"Code execution was cancelled by the user.\"\n",
        "    \n",
        "    print(\"   ‚è≥ Executing...\")\n",
        "    \n",
        "    try:\n",
        "        result = subprocess.run(\n",
        "            [sys.executable, \"-c\", code],\n",
        "            capture_output=True,\n",
        "            text=True,\n",
        "            timeout=30,\n",
        "            cwd=cwd\n",
        "        )\n",
        "        \n",
        "        output = \"\"\n",
        "        if result.stdout:\n",
        "            output += result.stdout\n",
        "        if result.stderr:\n",
        "            output += (\"\\n\" if output else \"\") + result.stderr\n",
        "        \n",
        "        if not output.strip():\n",
        "            output = \"OK (no output).\"\n",
        "        else:\n",
        "            output = output.strip()\n",
        "        \n",
        "        print(f\"   ‚úÖ Result:\")\n",
        "        print(\"‚îÄ\" * 40)\n",
        "        print(output)\n",
        "        print(\"‚îÄ\" * 40)\n",
        "        \n",
        "        # Include cwd in result so agent knows where files were saved\n",
        "        return f\"[Executed in: {cwd}]\\n{output}\"\n",
        "        \n",
        "    except subprocess.TimeoutExpired:\n",
        "        print(\"   ‚ùå Timed out after 30 seconds\")\n",
        "        return \"Error: Code execution timed out (30 second limit).\"\n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ùå Error: {str(e)}\")\n",
        "        return f\"Error: {str(e)}\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9417d7b",
      "metadata": {},
      "source": [
        "<span style=\"color:#ff5555;font-weight:bold;font-size:1.5rem;\">\n",
        "    STOP\n",
        "</span>\n",
        "\n",
        "... or keep going if you want to work ahead.\n",
        "\n",
        "---\n",
        "\n",
        "**Tools: memory**\n",
        "\n",
        "The agent needs to *use* the memory store we built. We expose four tools:\n",
        "- **memory_search** ‚Äì find memories by query (used automatically to inject context)\n",
        "- **memory_list** ‚Äì list recent memories (useful for ‚Äúwhat do you know about me?‚Äù)\n",
        "- **memory_upsert** ‚Äì create or update a memory (usually called by the memory manager, not the user)\n",
        "- **memory_delete** ‚Äì remove a memory by key\n",
        "\n",
        "The **memory manager** (a separate graph node) will decide what to upsert/delete based on the conversation; the main agent just reads via search/list and doesn‚Äôt have to manage storage itself.\n",
        "\n",
        "---\n",
        "\n",
        "<span style=\"color:#55ff55;font-weight:bold;font-size:1.5rem;\">\n",
        "    GO\n",
        "</span>\n",
        "\n",
        "The next cell defines the four memory tools. After that we‚Äôll add the MCP wrapper tools so the agent can call any connected MCP server."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77a352b3-bf0b-4412-82c1-a436b6544e3b",
      "metadata": {},
      "outputs": [],
      "source": [
        "@tool\n",
        "def memory_search(query: str, limit: int = 10) -> str:\n",
        "    \"\"\"Search persistent memory for relevant items.\"\"\"\n",
        "    print(f\"üß† [Memory Search] Searching for: \\\"{query}\\\"\")\n",
        "    results = MEM.search(query=query, limit=limit)\n",
        "    print(f\"   ‚úÖ Found {len(results)} matching memories\")\n",
        "    return json.dumps(results, indent=2)\n",
        "\n",
        "@tool\n",
        "def memory_list(limit: int = 50) -> str:\n",
        "    \"\"\"List recent persistent memories.\"\"\"\n",
        "    print(f\"üß† [Memory List] Listing up to {limit} memories\")\n",
        "    results = MEM.list_all(limit=limit)\n",
        "    print(f\"   ‚úÖ Retrieved {len(results)} memories\")\n",
        "    return json.dumps(results, indent=2)\n",
        "\n",
        "@tool\n",
        "def memory_upsert(key: str, value: str, tags: Optional[List[str]] = None) -> str:\n",
        "    \"\"\"Create/update a memory item.\"\"\"\n",
        "    print(f\"üß† [Memory Save] Saving: \\\"{key}\\\"\")\n",
        "    print(f\"   Value: {value[:100]}{'...' if len(value) > 100 else ''}\")\n",
        "    if tags:\n",
        "        print(f\"   Tags: {', '.join(tags)}\")\n",
        "    result = MEM.upsert(key=key, value=value, tags=tags)\n",
        "    print(f\"   ‚úÖ Memory saved successfully\")\n",
        "    return result\n",
        "\n",
        "@tool\n",
        "def memory_delete(key: str) -> str:\n",
        "    \"\"\"Delete a memory item by key.\"\"\"\n",
        "    print(f\"üß† [Memory Delete] Deleting: \\\"{key}\\\"\")\n",
        "    result = MEM.delete(key=key)\n",
        "    if \"No memory found\" in result:\n",
        "        print(f\"   ‚ö†Ô∏è  Memory not found\")\n",
        "    else:\n",
        "        print(f\"   ‚úÖ Memory deleted\")\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cca40777",
      "metadata": {},
      "source": [
        "<span style=\"color:#ff5555;font-weight:bold;font-size:1.5rem;\">\n",
        "    STOP\n",
        "</span>\n",
        "\n",
        "... or keep going if you want to work ahead.\n",
        "\n",
        "---\n",
        "\n",
        "**Tools: MCP wrappers**\n",
        "\n",
        "The agent doesn‚Äôt talk to MCP servers directly. We give it two tools:\n",
        "- **mcp_list_tools** ‚Äì list all tools from all connected servers (name, description, server).\n",
        "- **mcp_call** ‚Äì call a tool on a server by name with a JSON string of arguments.\n",
        "\n",
        "The model chooses the server and tool name from the list and passes arguments as a JSON string. We parse that and call `MCP.call_tool()` under the hood. After this, we‚Äôll collect all tools into one list and bind them to the LLM.\n",
        "\n",
        "---\n",
        "\n",
        "<span style=\"color:#55ff55;font-weight:bold;font-size:1.5rem;\">\n",
        "    GO\n",
        "</span>\n",
        "\n",
        "The next cell defines `mcp_call` and `mcp_list_tools`. The one after that sets `TOOLS = [web_search, python_interpreter, memory_*, mcp_*]`. Run both."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48ec10f7",
      "metadata": {},
      "outputs": [],
      "source": [
        "@tool\n",
        "def mcp_call(server: str, tool_name: str, arguments: str = \"{}\") -> str:\n",
        "    \"\"\"\n",
        "    Call a tool on a connected MCP server.\n",
        "    \n",
        "    Args:\n",
        "        server: Name of the MCP server (e.g., 'filesystem', 'github')\n",
        "        tool_name: Name of the tool to call\n",
        "        arguments: JSON string of arguments to pass to the tool\n",
        "    \"\"\"\n",
        "    print(f\"üîß [MCP] Calling {server}/{tool_name}\")\n",
        "    \n",
        "    try:\n",
        "        args = json.loads(arguments) if arguments else {}\n",
        "    except json.JSONDecodeError as e:\n",
        "        return f\"Invalid JSON arguments: {e}\"\n",
        "    \n",
        "    return run_async(MCP.call_tool(server, tool_name, args))\n",
        "\n",
        "@tool\n",
        "def mcp_list_tools() -> str:\n",
        "    \"\"\"List all available tools from connected MCP servers.\"\"\"\n",
        "    tools = MCP.get_all_tools_info()\n",
        "    \n",
        "    if not tools:\n",
        "        return \"No MCP servers connected. Use /mcp connect <name> <command> to connect.\"\n",
        "    \n",
        "    lines = [\"Available MCP tools:\"]\n",
        "    for t in tools:\n",
        "        desc = t['description'][:100] + '...' if len(t['description']) > 100 else t['description']\n",
        "        lines.append(f\"  ‚Ä¢ [{t['server']}] {t['name']}: {desc}\")\n",
        "    \n",
        "    return \"\\n\".join(lines)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30b50d07-9e69-425e-888a-ddb0bf94a59e",
      "metadata": {},
      "outputs": [],
      "source": [
        "TOOLS = [web_search, python_interpreter, memory_search, memory_list, memory_upsert, memory_delete, mcp_call, mcp_list_tools]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34b3b308",
      "metadata": {},
      "source": [
        "<span style=\"color:#ff5555;font-weight:bold;font-size:1.5rem;\">\n",
        "    STOP\n",
        "</span>\n",
        "\n",
        "... or keep going if you want to work ahead.\n",
        "\n",
        "---\n",
        "\n",
        "**LLM and tool calling**\n",
        "\n",
        "The agent is powered by a chat model (here, OpenAI) that supports **tool calls**: the model can return structured requests like ‚Äúcall `web_search` with query = ‚Ä¶‚Äù instead of only plain text. We define an `AgentState` (just `messages` for this graph), create the LLM with `make_llm()`, and **bind** the `TOOLS` list so the model knows their names and schemas. We‚Äôll use one LLM for the main agent and a smaller one for the memory manager.\n",
        "\n",
        "---\n",
        "\n",
        "<span style=\"color:#55ff55;font-weight:bold;font-size:1.5rem;\">\n",
        "    GO\n",
        "</span>\n",
        "\n",
        "The next cell defines `AgentState`, `make_llm`, `MAIN_LLM`, and `MEMORY_LLM`. Run it, then we‚Äôll add the system prompts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50b4d842-453b-43e8-a4df-3e63ba84addd",
      "metadata": {},
      "outputs": [],
      "source": [
        "class AgentState(TypedDict):\n",
        "    messages: List[Any]\n",
        "\n",
        "\n",
        "def _which_provider() -> str:\n",
        "    \"\"\"OpenAI over Gemini if both env vars set.\"\"\"\n",
        "    if os.getenv(\"OPENAI_API_KEY\"):\n",
        "        return \"openai\"\n",
        "    if os.getenv(\"GEMINI_API_KEY\"):\n",
        "        return \"gemini\"\n",
        "    raise ValueError(\n",
        "        \"Set OPENAI_API_KEY and/or GEMINI_API_KEY. \"\n",
        "        \"If both are set, OpenAI is used.\"\n",
        "    )\n",
        "\n",
        "\n",
        "def make_llm(\n",
        "    model: Optional[str] = None,\n",
        "    *,\n",
        "    openai_model: str = \"gpt-5-nano\",\n",
        "    gemini_model: str = \"gemini-3-flash-preview\",\n",
        ") -> BaseChatModel:\n",
        "    provider = _which_provider()\n",
        "    if provider == \"openai\":\n",
        "        m = model or openai_model\n",
        "        return ChatOpenAI(\n",
        "            model=m,\n",
        "            temperature=0.2\n",
        "        ).bind_tools(TOOLS)\n",
        "    else:\n",
        "        m = model or gemini_model\n",
        "        return ChatGoogleGenerativeAI(\n",
        "            model=m,\n",
        "            api_key=os.getenv(\"GEMINI_API_KEY\"),\n",
        "            temperature=0.2\n",
        "        ).bind_tools(TOOLS)\n",
        "\n",
        "\n",
        "_MAIN_OPENAI = \"gpt-5-mini\"\n",
        "_MAIN_GEMINI = \"gemini-3-flash-preview\"\n",
        "_MEMORY_OPENAI = \"gpt-5-nano\"\n",
        "_MEMORY_GEMINI = \"gemini-2.5-flash\"\n",
        "\n",
        "MAIN_LLM = make_llm(openai_model=_MAIN_OPENAI, gemini_model=_MAIN_GEMINI)\n",
        "MEMORY_LLM = make_llm(openai_model=_MEMORY_OPENAI, gemini_model=_MEMORY_GEMINI)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ceafba9",
      "metadata": {},
      "source": [
        "<span style=\"color:#ff5555;font-weight:bold;font-size:1.5rem;\">\n",
        "    STOP\n",
        "</span>\n",
        "\n",
        "... or keep going if you want to work ahead.\n",
        "\n",
        "---\n",
        "\n",
        "**System prompts**\n",
        "\n",
        "- **SYSTEM_MAIN** tells the agent its role (helpful agent with tools), when to use web search vs Python vs memory, and that it always has full conversation history. We‚Äôll also inject **retrieved memories** into this system message so the agent can personalize without storing everything in the prompt.\n",
        "- **SYSTEM_MEMORY** tells the **memory manager** to output JSON: a list of `upsert` and `delete` actions. Another node will parse that and call `MEM.upsert` / `MEM.delete`. The main agent doesn‚Äôt write to memory directly; the manager does it in one place.\n",
        "\n",
        "---\n",
        "\n",
        "<span style=\"color:#55ff55;font-weight:bold;font-size:1.5rem;\">\n",
        "    GO\n",
        "</span>\n",
        "\n",
        "The next cell defines `SYSTEM_MAIN` and `SYSTEM_MEMORY`. Run it, then we‚Äôll build the graph nodes and edges."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3026f46d-3a7c-453b-ae89-3a3032d4b87a",
      "metadata": {},
      "outputs": [],
      "source": [
        "SYSTEM_MAIN = SystemMessage(content=textwrap.dedent(f\"\"\"\n",
        "Today's date is {date.today()}\n",
        "Current working directory: {os.getcwd()}\n",
        "\n",
        "You are a helpful agent with tools. Keep your responses concise and to the point of what the user is asking for. Do not add any extra information or commentary.\n",
        "- Use web_search for fresh facts, citations, and \"what's current\".\n",
        "- Use python_interpreter for math, parsing, data transforms, and file operations.\n",
        "  - You can save files to the current directory or subdirectories like 'outputs/'\n",
        "- Use memory_search/list to recall persistent facts.\n",
        "- Do NOT store everything in memory. The memory manager will decide what to store.\n",
        "When you need a tool, call it via tool calling (not by describing it).\n",
        "\n",
        "IMPORTANT:\n",
        "- You are always given the full conversation history in the `messages` list.\n",
        "- Never claim you \"don't have the chat history\" or that it \"reset between interactions\" unless the user explicitly ran /reset.\n",
        "- If asked about prior turns (e.g., \"what was my last question?\"), answer by looking at the latest HumanMessage in `messages`.\n",
        "\"\"\").strip())\n",
        "\n",
        "\n",
        "SYSTEM_MEMORY = SystemMessage(content=textwrap.dedent(\"\"\"\n",
        "You are a MEMORY MANAGER for an assistant.\n",
        "\n",
        "Goal: maintain a small set of high-value, persistent facts about the user/preferences/projects.\n",
        "You may ADD, UPDATE, or DELETE memory items.\n",
        "\n",
        "Rules:\n",
        "- Only store info that will remain useful in future conversations (preferences, stable project facts, recurring constraints).\n",
        "- Avoid saving transient details (one-off tasks, short-lived plans, temporary numbers).\n",
        "- If a memory becomes wrong/outdated, delete or update it.\n",
        "- Output MUST be valid JSON in this schema:\n",
        "\n",
        "{\n",
        "  \"actions\": [\n",
        "    {\"type\": \"upsert\", \"key\": \"...\", \"value\": \"...\", \"tags\": [\"...\"]},\n",
        "    {\"type\": \"delete\", \"key\": \"...\"}\n",
        "  ]\n",
        "}\n",
        "\n",
        "If no changes: {\"actions\": []}\n",
        "\"\"\").strip())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eedf5732",
      "metadata": {},
      "source": [
        "<span style=\"color:#ff5555;font-weight:bold;font-size:1.5rem;\">\n",
        "    STOP\n",
        "</span>\n",
        "\n",
        "... or keep going if you want to work ahead.\n",
        "\n",
        "---\n",
        "\n",
        "**Graph nodes and streaming**\n",
        "\n",
        "We'll implement two graph nodes and supporting helpers:\n",
        "1. **main_agent_node** ‚Äì Takes `messages`, optionally injects retrieved memories into the system message, calls the main LLM. If the LLM returns tool calls, we route to `tools`.\n",
        "2. **tools_node** ‚Äì Runs each tool call from the last AI message, appends `ToolMessage` results to `messages`, then we go back to **main** so the agent can reason on the results.\n",
        "\n",
        "**Conditional edge:** After `main`, we check the last message: if it has `tool_calls`, go to `tools`; otherwise the graph ends.\n",
        "\n",
        "**Memory manager (background):** After the graph finishes, `memory_manager_node` runs in a **background thread** so it never blocks you from typing the next message. It calls a smaller LLM on the last few turns and applies any upsert/delete actions to the memory store.\n",
        "\n",
        "**Streaming helper:** `_stream_agent` uses LangGraph's `astream_events` to print the agent's response **token by token** as it's generated, rather than waiting for the full response.\n",
        "\n",
        "---\n",
        "\n",
        "<span style=\"color:#55ff55;font-weight:bold;font-size:1.5rem;\">\n",
        "    GO\n",
        "</span>\n",
        "\n",
        "The next cell defines `retrieve_relevant_memories`, `main_agent_node`, `tools_node`, `should_continue`, `memory_manager_node`, `_run_memory_bg`, and `_stream_agent`. Run it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c68efd1e-36ec-462c-a033-af1d7f26eef7",
      "metadata": {},
      "outputs": [],
      "source": [
        "def retrieve_relevant_memories(user_message: str) -> str:\n",
        "    \"\"\"\n",
        "    Automatically retrieve relevant memories based on the user's message.\n",
        "    Uses multiple search strategies for robustness.\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    \n",
        "    direct_results = MEM.search(user_message, limit=5)\n",
        "    results.extend(direct_results)\n",
        "    \n",
        "    personal_keywords = [\"name\", \"preference\", \"project\", \"work\", \"like\", \"favorite\", \"setting\"]\n",
        "    for keyword in personal_keywords:\n",
        "        if keyword.lower() in user_message.lower():\n",
        "            keyword_results = MEM.search(keyword, limit=3)\n",
        "            for r in keyword_results:\n",
        "                if r not in results:\n",
        "                    results.append(r)\n",
        "    \n",
        "    recent = MEM.list_all(limit=10)\n",
        "    for r in recent:\n",
        "        if r not in results:\n",
        "            results.append(r)\n",
        "    \n",
        "    seen_keys = set()\n",
        "    unique_results = []\n",
        "    for r in results:\n",
        "        if r[\"key\"] not in seen_keys:\n",
        "            seen_keys.add(r[\"key\"])\n",
        "            unique_results.append(r)\n",
        "    \n",
        "    if not unique_results:\n",
        "        return \"\"\n",
        "    \n",
        "    memory_text = \"=== RETRIEVED MEMORIES (use this information to personalize your response) ===\\n\"\n",
        "    for mem in unique_results[:15]:\n",
        "        tags_str = f\" [tags: {', '.join(mem['tags'])}]\" if mem['tags'] else \"\"\n",
        "        memory_text += f\"- {mem['key']}: {mem['value']}{tags_str}\\n\"\n",
        "    memory_text += \"=== END MEMORIES ===\\n\"\n",
        "    \n",
        "    return memory_text\n",
        "\n",
        "\n",
        "def main_agent_node(state: AgentState) -> AgentState:\n",
        "    msgs = state[\"messages\"]\n",
        "    \n",
        "    latest_user_msg = None\n",
        "    for m in reversed(msgs):\n",
        "        if isinstance(m, HumanMessage):\n",
        "            latest_user_msg = normalize_ai_content(m.content)\n",
        "            break\n",
        "    \n",
        "    memory_context = \"\"\n",
        "    if latest_user_msg:\n",
        "        memory_context = retrieve_relevant_memories(latest_user_msg)\n",
        "    \n",
        "    system_content = SYSTEM_MAIN.content\n",
        "    if memory_context:\n",
        "        system_content = f\"{system_content}\\n\\n{memory_context}\"\n",
        "    \n",
        "    working_msgs = msgs.copy()\n",
        "    if not working_msgs or not isinstance(working_msgs[0], SystemMessage):\n",
        "        working_msgs = [SystemMessage(content=system_content)] + working_msgs\n",
        "    else:\n",
        "        working_msgs[0] = SystemMessage(content=system_content)\n",
        "\n",
        "    ai = MAIN_LLM.invoke(working_msgs)\n",
        "    return {\"messages\": msgs + [ai]}\n",
        "\n",
        "\n",
        "def tools_node(state: AgentState) -> AgentState:\n",
        "    \"\"\"Executes any tool calls found in the last AI message.\"\"\"\n",
        "    msgs = state[\"messages\"]\n",
        "    last = msgs[-1]\n",
        "    if not isinstance(last, AIMessage):\n",
        "        return state\n",
        "\n",
        "    tool_messages: List[ToolMessage] = []\n",
        "    for call in (last.tool_calls or []):\n",
        "        name = call.get(\"name\")\n",
        "        args = call.get(\"args\", {})\n",
        "        tool_map = {t.name: t for t in TOOLS}\n",
        "        tool_obj = tool_map.get(name)\n",
        "        if tool_obj is None:\n",
        "            tool_messages.append(ToolMessage(content=f\"Unknown tool: {name}\", tool_call_id=call.get(\"id\", \"unknown\")))\n",
        "            continue\n",
        "        result = tool_obj.invoke(args)\n",
        "        tool_messages.append(ToolMessage(content=str(result), tool_call_id=call.get(\"id\", \"tool_call\")))\n",
        "    return {\"messages\": msgs + tool_messages}\n",
        "\n",
        "def should_continue(state: AgentState) -> str:\n",
        "    last = state[\"messages\"][-1]\n",
        "    if isinstance(last, AIMessage) and getattr(last, \"tool_calls\", None):\n",
        "        return \"tools\"\n",
        "    return \"end\"\n",
        "\n",
        "def _run_memory_bg(messages):\n",
        "    \"\"\"Run memory manager in a background thread so it never blocks the REPL.\"\"\"\n",
        "    try:\n",
        "        memory_manager_node({\"messages\": messages})\n",
        "    except Exception:\n",
        "        pass  # don't crash on memory failures\n",
        "\n",
        "def memory_manager_node(state: AgentState) -> AgentState:\n",
        "    \"\"\"\n",
        "    After the main agent responds, decide what to add/remove in memory,\n",
        "    then apply it via memory_upsert/memory_delete tools.\n",
        "    \"\"\"\n",
        "    msgs = state[\"messages\"]\n",
        "\n",
        "    tail = msgs[-12:]\n",
        "    mm_input = [SYSTEM_MEMORY] + tail\n",
        "\n",
        "    mm = MEMORY_LLM.invoke(mm_input)\n",
        "\n",
        "    actions = []\n",
        "    try:\n",
        "        payload = json.loads(normalize_ai_content(mm.content))\n",
        "        actions = payload.get(\"actions\", [])\n",
        "    except Exception:\n",
        "        return state\n",
        "\n",
        "    for act in actions:\n",
        "        if act.get(\"type\") == \"upsert\":\n",
        "            MEM.upsert(\n",
        "                key=act[\"key\"],\n",
        "                value=act[\"value\"],\n",
        "                tags=act.get(\"tags\") or [],\n",
        "            )\n",
        "        elif act.get(\"type\") == \"delete\":\n",
        "            MEM.delete(key=act[\"key\"])\n",
        "\n",
        "    return state\n",
        "\n",
        "async def _stream_agent(history):\n",
        "    \"\"\"Stream the agent's text output token by token.\"\"\"\n",
        "    streamed_any = False\n",
        "    final_messages = None\n",
        "\n",
        "    async for event in APP.astream_events({\"messages\": history}, version=\"v2\"):\n",
        "        kind = event[\"event\"]\n",
        "        node = event.get(\"metadata\", {}).get(\"langgraph_node\", \"\")\n",
        "\n",
        "        if kind == \"on_chat_model_stream\" and node == \"main\":\n",
        "            chunk = event[\"data\"][\"chunk\"]\n",
        "            text = \"\"\n",
        "            if hasattr(chunk, \"content\"):\n",
        "                c = chunk.content\n",
        "                if isinstance(c, str):\n",
        "                    text = c\n",
        "                elif isinstance(c, list):\n",
        "                    text = \"\".join(\n",
        "                        p.get(\"text\", \"\") if isinstance(p, dict) else str(p)\n",
        "                        for p in c\n",
        "                    )\n",
        "            if text:\n",
        "                if not streamed_any:\n",
        "                    sys.stdout.write(\"agent> \")\n",
        "                    streamed_any = True\n",
        "                sys.stdout.write(text)\n",
        "                sys.stdout.flush()\n",
        "\n",
        "        if kind == \"on_chain_end\" and event[\"name\"] == \"LangGraph\":\n",
        "            output = event[\"data\"].get(\"output\", {})\n",
        "            if isinstance(output, dict) and \"messages\" in output:\n",
        "                final_messages = output[\"messages\"]\n",
        "\n",
        "    if streamed_any:\n",
        "        sys.stdout.write(\"\\n\\n\")\n",
        "        sys.stdout.flush()\n",
        "    else:\n",
        "        print(\"agent> (no response)\\n\")\n",
        "\n",
        "    # Fire off memory manager in background ‚Äî user can type immediately\n",
        "    if final_messages:\n",
        "        threading.Thread(\n",
        "            target=_run_memory_bg,\n",
        "            args=(list(final_messages),),\n",
        "            daemon=True,\n",
        "        ).start()\n",
        "\n",
        "    return final_messages if final_messages else history"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bdbc4087",
      "metadata": {},
      "source": [
        "<span style=\"color:#ff5555;font-weight:bold;font-size:1.5rem;\">\n",
        "    STOP\n",
        "</span>\n",
        "\n",
        "... or keep going if you want to work ahead.\n",
        "\n",
        "---\n",
        "\n",
        "**Assembling the graph**\n",
        "\n",
        "We create a `StateGraph(AgentState)`, add the two nodes, set the entry point to `main`, and add:\n",
        "- **Conditional edge** from `main` ‚Üí `should_continue` ‚Üí `tools` or `END`\n",
        "- **Edge** `tools` ‚Üí `main` (loop until no more tool calls)\n",
        "\n",
        "Then we **compile** the graph into a runnable `APP`. When we stream through `APP`, it runs the full loop: agent ‚Üí tools (if any) ‚Üí back to agent ‚Üí ‚Ä¶ ‚Üí end. The memory manager fires off in a background thread after each turn.\n",
        "\n",
        "---\n",
        "\n",
        "<span style=\"color:#55ff55;font-weight:bold;font-size:1.5rem;\">\n",
        "    GO\n",
        "</span>\n",
        "\n",
        "The next cell builds the graph and compiles it to `APP`. Run it, then we'll add the chat loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2062e28e-fa1b-4a84-8890-6ab4db022ebe",
      "metadata": {},
      "outputs": [],
      "source": [
        "graph = StateGraph(AgentState)\n",
        "graph.add_node(\"main\", main_agent_node)\n",
        "graph.add_node(\"tools\", tools_node)\n",
        "\n",
        "graph.set_entry_point(\"main\")\n",
        "graph.add_conditional_edges(\"main\", should_continue, {\n",
        "    \"tools\": \"tools\",\n",
        "    \"end\": END,\n",
        "})\n",
        "graph.add_edge(\"tools\", \"main\")\n",
        "\n",
        "APP = graph.compile()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2aea8eb9",
      "metadata": {},
      "source": [
        "<span style=\"color:#ff5555;font-weight:bold;font-size:1.5rem;\">\n",
        "    STOP\n",
        "</span>\n",
        "\n",
        "... or keep going if you want to work ahead.\n",
        "\n",
        "---\n",
        "\n",
        "**Chat loop**\n",
        "\n",
        "The last piece is a simple REPL: read a line from the user, handle commands like `/exit`, `/reset`, `/mem`, `/mcp`, `/env`, `/img`, then build a `HumanMessage` (with any queued images), append it to `history`, and **stream** the agent's response token by token via `_stream_agent`. The memory manager runs automatically in a background thread at the end of each turn, so you can start typing your next message immediately.\n",
        "\n",
        "---\n",
        "\n",
        "<span style=\"color:#55ff55;font-weight:bold;font-size:1.5rem;\">\n",
        "    GO\n",
        "</span>\n",
        "\n",
        "Run the cell below to start chatting with your agent. Try questions that need search, memory, or code execution, and use `/img <path>` to attach images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d8b1e46-3a3c-42f0-be24-5a2e7feb209a",
      "metadata": {
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "print(\"Chat with your Agent. Commands: /exit, /reset, /mem, /mcp, /env, /img\")\n",
        "print(\"‚ö†Ô∏è  WARNING: Python execution is UNRESTRICTED. Use caution with code execution requests.\\n\")\n",
        "history: List[Any] = []\n",
        "pending_images: List[str] = []\n",
        "\n",
        "while True:\n",
        "    user = input(\"you> \").strip()\n",
        "    if not user:\n",
        "        continue\n",
        "    if user.lower() in {\"/exit\", \"/quit\"}:\n",
        "        break\n",
        "    if user.lower() == \"/reset\":\n",
        "        history = []\n",
        "        pending_images = []\n",
        "        print(\"(cleared)\\n\")\n",
        "        continue\n",
        "    if user.lower() == \"/mem\":\n",
        "        print(memory_list.invoke({\"limit\": 25}))\n",
        "        print()\n",
        "        continue\n",
        "    \n",
        "    if user.lower().startswith(\"/img\"):\n",
        "        parts = user.split(maxsplit=1)\n",
        "        if len(parts) < 2:\n",
        "            print(\"Image Commands:\")\n",
        "            print(\"  /img <path>    - Attach an image to your next message\")\n",
        "            print(\"  /img clear     - Clear pending images\")\n",
        "            print(\"  /img list      - Show pending images\")\n",
        "            print(f\"\\nPending images: {len(pending_images)}\")\n",
        "            for p in pending_images:\n",
        "                print(f\"  üì∑ {p}\")\n",
        "            print(\"\\nSupported formats: jpg, png, gif, webp\")\n",
        "            print(\"Example: /img ~/photos/screenshot.png\")\n",
        "        else:\n",
        "            arg = parts[1].strip()\n",
        "            if arg.lower() == \"clear\":\n",
        "                pending_images = []\n",
        "                print(\"‚úÖ Cleared pending images\")\n",
        "            elif arg.lower() == \"list\":\n",
        "                if pending_images:\n",
        "                    print(f\"Pending images ({len(pending_images)}):\")\n",
        "                    for p in pending_images:\n",
        "                        print(f\"  üì∑ {p}\")\n",
        "                else:\n",
        "                    print(\"No pending images\")\n",
        "            else:\n",
        "                img_path = Path(arg).expanduser().resolve()\n",
        "                if img_path.exists():\n",
        "                    pending_images.append(str(img_path))\n",
        "                    print(f\"‚úÖ Image queued: {img_path}\")\n",
        "                    print(f\"   ({len(pending_images)} image(s) will be sent with your next message)\")\n",
        "                else:\n",
        "                    print(f\"‚ùå File not found: {img_path}\")\n",
        "        print()\n",
        "        continue\n",
        "    \n",
        "    if user.lower().startswith(\"/env\"):\n",
        "        parts = user.split(maxsplit=2)\n",
        "        \n",
        "        if len(parts) == 1:\n",
        "            print(\"Environment Variables:\")\n",
        "            print(\"  /env KEY=VALUE       - Set an environment variable\")\n",
        "            print(\"  /env KEY             - Show current value of KEY\")\n",
        "            print(\"  /env list            - List all custom-set vars this session\")\n",
        "            print(\"\\nCommon variables for MCP servers:\")\n",
        "            print(f\"  GITHUB_TOKEN = {'[SET]' if os.environ.get('GITHUB_TOKEN') else '[NOT SET]'}\")\n",
        "            print(f\"  OPENAI_API_KEY = {'[SET]' if os.environ.get('OPENAI_API_KEY') else '[NOT SET]'}\")\n",
        "            print(f\"  ANTHROPIC_API_KEY = {'[SET]' if os.environ.get('ANTHROPIC_API_KEY') else '[NOT SET]'}\")\n",
        "            print(\"\\nExample: /env GITHUB_TOKEN=ghp_xxxxxxxxxxxx\")\n",
        "        elif len(parts) >= 2:\n",
        "            arg = parts[1] if len(parts) == 2 else parts[1] + \" \" + parts[2]\n",
        "            \n",
        "            if arg.lower() == \"list\":\n",
        "                print(\"Current environment (selected vars):\")\n",
        "                for key in ['GITHUB_TOKEN', 'OPENAI_API_KEY', 'ANTHROPIC_API_KEY', 'TAVILY_API_KEY', 'GEMINI_API_KEY']:\n",
        "                    val = os.environ.get(key)\n",
        "                    if val:\n",
        "                        masked = val[:4] + '...' + val[-4:] if len(val) > 10 else '[SET]'\n",
        "                        print(f\"  {key} = {masked}\")\n",
        "                    else:\n",
        "                        print(f\"  {key} = [NOT SET]\")\n",
        "            elif \"=\" in arg:\n",
        "                key, value = arg.split(\"=\", 1)\n",
        "                key = key.strip()\n",
        "                value = value.strip()\n",
        "                os.environ[key] = value\n",
        "                masked = value[:4] + '...' + value[-4:] if len(value) > 10 else value\n",
        "                print(f\"‚úÖ Set {key} = {masked}\")\n",
        "            else:\n",
        "                key = arg.strip()\n",
        "                val = os.environ.get(key)\n",
        "                if val:\n",
        "                    masked = val[:4] + '...' + val[-4:] if len(val) > 10 else val\n",
        "                    print(f\"{key} = {masked}\")\n",
        "                else:\n",
        "                    print(f\"{key} = [NOT SET]\")\n",
        "        print()\n",
        "        continue\n",
        "    \n",
        "    if user.lower().startswith(\"/mcp\"):\n",
        "        parts = user.split(maxsplit=3)\n",
        "        cmd = parts[1] if len(parts) > 1 else \"help\"\n",
        "        \n",
        "        if cmd == \"list\":\n",
        "            print(MCP.list_servers())\n",
        "        elif cmd == \"tools\":\n",
        "            print(mcp_list_tools.invoke({}))\n",
        "        elif cmd == \"connect\" and len(parts) >= 3:\n",
        "            name = parts[2]\n",
        "            if len(parts) > 3:\n",
        "                cmd_parts = parts[3].split()\n",
        "                command = cmd_parts[0]\n",
        "                args = cmd_parts[1:] if len(cmd_parts) > 1 else []\n",
        "            else:\n",
        "                print(\"Usage: /mcp connect <name> <command> [args...]\")\n",
        "                print(\"Example: /mcp connect filesystem npx -y @modelcontextprotocol/server-filesystem /path/to/dir\")\n",
        "                continue\n",
        "            print(run_async(MCP.connect(name, command, args)))\n",
        "        elif cmd == \"disconnect\" and len(parts) >= 3:\n",
        "            print(run_async(MCP.disconnect(parts[2])))\n",
        "        else:\n",
        "            print(\"MCP Commands:\")\n",
        "            print(\"  /mcp list              - List connected servers\")\n",
        "            print(\"  /mcp tools             - List available tools from all servers\")\n",
        "            print(\"  /mcp connect <name> <command> [args...]  - Connect to a server\")\n",
        "            print(\"  /mcp disconnect <name> - Disconnect from a server\")\n",
        "            print(\"\\nExamples:\")\n",
        "            print(\"  /env GITHUB_TOKEN=ghp_xxxxxxxxxxxx\")\n",
        "            print(\"  /mcp connect github npx -y @modelcontextprotocol/server-github\")\n",
        "            print(\"  /mcp connect fs npx -y @modelcontextprotocol/server-filesystem /Users/me/projects\")\n",
        "        print()\n",
        "        continue\n",
        "\n",
        "    if pending_images:\n",
        "        human_msg = create_multimodal_message(user, pending_images)\n",
        "        pending_images = []\n",
        "    else:\n",
        "        human_msg = HumanMessage(content=user)\n",
        "    \n",
        "    history.append(human_msg)\n",
        "    history = run_async(_stream_agent(history))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee113681",
      "metadata": {},
      "source": [
        "<span style=\"color:#ff5555;font-weight:bold;font-size:1.5rem;\">\n",
        "    STOP\n",
        "</span>\n",
        "\n",
        "---\n",
        "\n",
        "**Where to go from here**\n",
        "\n",
        "- **Add more tools** ‚Äì e.g. database queries, email, custom APIs.\n",
        "- **Connect MCP servers** ‚Äì `/env GITHUB_TOKEN=...` then `/mcp connect github npx -y @modelcontextprotocol/server-github` to give the agent GitHub access.\n",
        "- **Tighten safety** ‚Äì sandbox the Python interpreter, rate-limit search, or restrict which MCP servers can be connected.\n",
        "- **Improve memory** ‚Äì try semantic search over memories instead of simple text search (try checking out our embeddings workshop from last year ;) https://github.com/MSOE-AI-Club/workshops/blob/main/Embeddings/embeddings-workshop.ipynb).\n",
        "- **Multi-agent** ‚Äì use LangGraph to add specialist agents (e.g. researcher vs coder) with different tools and prompts.\n",
        "\n",
        "You‚Äôve built a full agent with tools, memory, MCP, and a clear STOP/GO workshop flow. Have fun extending it!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
