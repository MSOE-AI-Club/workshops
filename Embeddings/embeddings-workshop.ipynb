{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embeddings\n",
    "===\n",
    "MAIC - Spring, Week 4<br>\n",
    "```\n",
    "  _____________\n",
    " /0   /     \\  \\\n",
    "/  \\ M A I C/  /\\\n",
    "\\ / *      /  / /\n",
    " \\___\\____/  @ /\n",
    "          \\_/_/\n",
    "```\n",
    "(Rosie is not needed!)\n",
    "\n",
    "Prereqs:\n",
    "- Install [VSCode](https://code.visualstudio.com/)\n",
    "- Install [Python](https://www.python.org/downloads/)\n",
    "- Ensure you can run notebooks in VSCode.\n",
    "\n",
    "<span style=\"color:#ff5555;font-weight:bold;font-size:1.5rem;\">\n",
    "    STOP\n",
    "</span>\n",
    "\n",
    "... or keep going if you want to work ahead.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embeddings are an extremely useful tool in modern machine learning, allowing raw text to be transformed into numerical representations that models can understand.\n",
    "They are also a popular interview question to test a candidateâ€™s understanding of vector spaces, similarity metrics, and real-world applications.\n",
    "Beyond that, embeddings are incredibly common in ML, powering everything from search engines and recommendation systems to chatbots and fraud detection.\n",
    "You'll see embeddings being used everywhere if you look! Here are just some models, projects, and papers that make use of embeddings:\n",
    "- [The original transformer paper - the basis of modern LLMs](https://arxiv.org/pdf/1706.03762)\n",
    "- [RAG systems - often used to give LLMs comprehensive access to much more information than they could normally use at once](https://en.wikipedia.org/wiki/Retrieval-augmented_generation)\n",
    "- [Image generations models such as Stable Diffusion](https://en.wikipedia.org/wiki/Stable_Diffusion)\n",
    "  - Note: images are generated *in embedding space*!\n",
    "- [Audio-continuation models such as RAVE](https://github.com/acids-ircam/RAVE)\n",
    "- Modern image search makes extensive use of embeddings\n",
    "- Modern recommendation algorithms also use embeddings\n",
    "- Even some papers published by MSOE students involve the use of embeddings! Here are a few:\n",
    "  - [Agent simulation with LLMs](https://arxiv.org/pdf/2409.13753)\n",
    "  - [Strategy masking - a technique to control model behavior](https://arxiv.org/pdf/2501.05501)\n",
    "\n",
    "**What *are* Embeddings?**\n",
    "\n",
    "At the lowest level, an embedding is just stored as a list of numbers. This could be an embedding: `[0.1, 0.2, -0.3]`.\n",
    "\n",
    "This list of numbers is best interpreted as a point or direction in some very high-dimensional space that represents something. In the case of text-based models, embeddings are used to represent words and sentences.\n",
    "\n",
    "In practice, embeddings range from tens of dimensions to over 1000. For simplicity, let's only conceptualize things in two or three dimensions for now - that way we can actually visualize what's going on.\n",
    "\n",
    "The image below shows how embedded words of the phrase `some embedded text` can be thought of as directions in space - the embedding point describes direction relative to the point (0,0).\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/MSOE-AI-Club/workshops/refs/heads/main/Embeddings/img1.png\" width=1000px>\n",
    "\n",
    "But how do we actually interpret these directions in space as being words? The answer is that different directions in the space represent different aspects of a word -\n",
    "- one direction may encode \"past tense,\"\n",
    "- another may enode the idea of \"running\" or \"to run.\"\n",
    "\n",
    "In the case above, the embedding of the word \"ran\" may point in the average of the directions encoding \"past tense,\" and \"to run.\"\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/MSOE-AI-Club/workshops/refs/heads/main/Embeddings/img2.png\" width=600px>\n",
    "\n",
    "This topic naturally leads into another important point: embeddings *closer* in embedding space are also *closer* in meaning. The word \"ran\" will be closer to \"walked\" than to \"stapler.\" This is the case, because words with increasingly different meanings are, *by definition,* pointing in increasingly different directions to encode those meanings.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/MSOE-AI-Club/workshops/refs/heads/main/Embeddings/img3.png\" width=600px>\n",
    "\n",
    "NOTE: we'll watch this during the workshop:\n",
    "\n",
    "[Here is a one-minute video that illustrates this concept using real-world embeddings.](https://www.youtube.com/watch?v=FJtFZwbvkI4)\n",
    "\n",
    "**How is it possible for there to be directions dedicated to ideas as specific as \"Italian-ness\" and \"WWII Axis leaders?\"**\n",
    "\n",
    "One might expect that the directions in embedding space would represent more general concepts.\n",
    "\n",
    "If directions can be allocated to specific ideas like \"WWII Axis leaders,\" how are there enough directions left to represent everything else, from \"60s British pop bands\" to \"computer keyboard layouts\"?!?!\n",
    "\n",
    "In two or three dimensions, it's *not* really possible to have directions this specific. But, remember that text embeddings are typically 10s to 1000s of dimensions.  \n",
    "\n",
    "As the number of dimensions grows, the number of possible points and directions in a space grows MUCH more quickly.\n",
    "\n",
    "Let's work with the constraint that points of different meanings must be one unit apart. This is somewhat arbitrary, but it is true that there is a \"minumum\" distance between two points before they mean the same thing. Let's also say that we only allow points in the range 0 to 1. This is also somewhat arbitrary, but machine learning models often try to keep numbers from getting too big to prevent numbers from going to infinity. With these constraints, we can only fit two points in one dimension:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/MSOE-AI-Club/workshops/refs/heads/main/Embeddings/img4.png\" width=600px>\n",
    "\n",
    "These two points (or two directions relative to a centerpoint) probably can't encode much information. But what if we extrude ourselves into the second dimension with the same constraints? \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/MSOE-AI-Club/workshops/refs/heads/main/Embeddings/img5.png\" width=600px>\n",
    "\n",
    "We now have *four* points (or four directions). And if we went to three dimensions we'd have eight points - imagine extruding the four points of this square into a cube. In general, our constraints will allow N dimensions to encode $2^N$ unique directions.\n",
    "\n",
    "- Only 10 dimensions gets us over 1000 directions.\n",
    "- 20 dimensions gets us over 1 million directions.\n",
    "- And at 1000 dimensions, we have **more possible unique directions than atoms in the observable universe,** each of which can be interpolated between to embed specific words or sentences!\n",
    "\n",
    "The act of adding just *one* dimension EXPONENTIALLY increases how many things we can fit in the space! So think about adding a dimension to a 3D space... *1000 times*.\n",
    "\n",
    "<img src=\"https://www.i2tutorials.com/wp-content/media/2019/09/Curse-of-Dimensionality-i2tutorials.png\" width=1000px>\n",
    "\n",
    "Although we can't *see* the directions encoding things like \"WWII Axis leaders,\" there is no doubt that these directions are able to exist.\n",
    "\n",
    "**Who decided that there should be directions for these particular ideas?**\n",
    "\n",
    "These directions are not something humans designed directly. Instead, these directions *emerge* from the process of training the model.\n",
    "\n",
    "The model learns from a huge amount of text and starts to recognize patterns, like which words tend to appear in similar contexts.\n",
    "\n",
    "As it processes more and more language, the model \"figures out\" what sort of information it should store in the directions of an embedding space - even though no one explicitly programmed it to do that!\n",
    "\n",
    "---\n",
    "\n",
    "<span style=\"color:#55ff55;font-weight:bold;font-size:1.5rem;\">\n",
    "    GO\n",
    "</span>\n",
    "\n",
    "**That seems neat. How can *I* use embeddings?**\n",
    "\n",
    "Let's set things up!\n",
    "\n",
    "It's really easy to get started with embeddings. You can even run small embedding models on your laptop!\n",
    "\n",
    "We'll be using `sentence-transformers` to run [all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) - a model that embeds sentences into 384 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shouldn't take longer that 5 mins\n",
    "%pip install sentence-transformers\n",
    "%pip install tf-keras\n",
    "%pip instal numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\") # Our model of choice is supplied here. You can find many more on huggingface: https://huggingface.co/models?sort=trending&search=embed\n",
    "embedding = model.encode(\"This is an embedded text example.\")\n",
    "\n",
    "print(embedding) # our embeddings are just lists of numbers stored as Numpy arrays. Numpy is just a library that makes it easier to manipulate arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#ff5555;font-weight:bold;font-size:1.5rem;\">\n",
    "    STOP\n",
    "</span>\n",
    "\n",
    "... or keep going if you want to work ahead.\n",
    "\n",
    "---\n",
    "\n",
    "Now that we (hopefully) have a working embedding model, let's put it to use.\n",
    "\n",
    "Let's first try the example from the previously linked 3Blue1Brown video. But before that, we need to understand how to measure \"distance\" in embedding space.\n",
    "\n",
    "[TODO - explain cosine similarity]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_uncle = model.encode(\"uncle\")\n",
    "emb_aunt = model.encode(\"aunt\")\n",
    "emb_man = model.encode(\"man\")\n",
    "emb_woman = model.encode(\"woman\")\n",
    "\n",
    "# ^ visualize?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.73387706"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(emb_uncle, emb_aunt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98456275"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(emb_uncle - emb_man + emb_woman, emb_aunt) # higher sim!!!\n",
    "\n",
    "# If you're done and still waiting: work activity: what about Hitler? Other terms?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO\n",
    "\n",
    "more on visualizing embeddings\n",
    "- bar plot\n",
    "- img - 100 pixels in an img are visually a lot more info than 2d direction\n",
    "- pca\n",
    "\n",
    "some search task\n",
    "\n",
    "some clustering task\n",
    "\n",
    "maybe: high level of embeddings in transformer/attention?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
